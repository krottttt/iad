{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmsFABwClrsS"
   },
   "source": [
    "# Домашняя работа\n",
    "\n",
    "В этой работе вам предстоит с помощью encoder-decoder архитектуры, пробуя различные ее реализации, решить задачу машинного перевода.\n",
    "\n",
    "#### Наша задача - сделать свой собственный переводчик!\n",
    "\n",
    "Пока что только русско-английский:) Будем учиться на текстах описания отелей, так что при успешном выполнении этого задания у вас не возникнет проблем с выбором места для остановки в путешествии, так как все отзывы вам будут высококлассно переведены!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что необходимо обсудить до начала работы?\n",
    " \n",
    "*как токенезовать и закодировать текст?*\n",
    "\n",
    "С токенезацией хорошо справится WordPunctTokenizer из библиотеки nltk, а вот с кодированием не все так просто, как может показаться... \n",
    "\n",
    "В наших текстах очень много редких и очень мало встречаемых слов (в каждом отеле есть своя фишка: какой-то предмет декорации или услуга, которая описывается своим словом, которое только там и встречается). Если мы будем кодировать все слова, то размер нашего словаря будет очень-очень большим.\n",
    "\n",
    "Но на одном из семинаров мы кодировали побуквенно, кажется, что тут это может помочь! Да, действительно так, но придется очень очень долго обучать модель, а путешествовать и выбрать хороший отель уже хочется, поэтому мы придем к чему-то среднему между этими подходами -  [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) известный как __BPE__\n",
    "\n",
    "Этот алгоритм стартует с посимвольного уровня и итеративно мерджит самые встречаемые пары. И так N итераций. На выходе мы получаем самые частые последовательности символов из которых формируются слова!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE - очень популярный и частоиспользуемый алгоритм в задачах NLP, поэтому есть много открытых реализаций этого алгоритма\n",
    "\n",
    "Мы уверены, что вы научились гуглить и искать полезные материалы в интернете, когда делали домашнее задание по YOLO, поэтому в этот раз просто покажем один из способов, как это можно сделать и затем в своих проектах вы можете брать этот подход и, возможно, как-то улучшать его!\n",
    "\n",
    "Тем кому очень интересно, как же все работает - заходите в файл vocab.py, очень советуем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: C:\\Users\\playi\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: subword_nmt in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from subword_nmt) (4.64.1)\n",
      "Requirement already satisfied: mock in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from subword_nmt) (5.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\playi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->subword_nmt) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: C:\\Users\\playi\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install subword_nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9kP0SdxlrsY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:09<00:00, 822.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:10<00:00, 773.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "def tokenize(x):\n",
    "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
    "\n",
    "# разбиваем и токенизируем тексты, записываем обработанные токены в файл\n",
    "with open('train.en', 'w',encoding = 'utf8') as f_src,  open('train.ru', 'w',encoding = 'utf8') as f_dst:\n",
    "    for line in open('data.txt',encoding = 'utf8'):\n",
    "        src_line, dst_line = line.strip().split('\\t')\n",
    "        f_src.write(tokenize(src_line) + '\\n')\n",
    "        f_dst.write(tokenize(dst_line) + '\\n')\n",
    "\n",
    "# строим и применяем bpe кодирование\n",
    "bpe = {}\n",
    "for lang in ['en', 'ru']:\n",
    "    learn_bpe(open('./train.' + lang,encoding = 'utf8'), open('bpe_rules.' + lang, 'w',encoding = 'utf8'), num_symbols=8000)\n",
    "    bpe[lang] = BPE(open('./bpe_rules.' + lang,encoding = 'utf8'))\n",
    "    \n",
    "    with open('train.bpe.' + lang, 'w',encoding = 'utf8') as f_out:\n",
    "        for line in open('train.' + lang,encoding = 'utf8'):\n",
    "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение словарей, разбиение данных\n",
    "\n",
    "Сейчас, когда мы обучили BPE алгоритм на наших данных, построим словарь соответствия токена и его индекса, чтобы нам было затем удобно смотреть переводы и переводить новые предложения\n",
    "\n",
    "Также сделаем разбиение на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmTy_m_olrsb"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PskgBSxlrsd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: дом с проживанием в семье lake front расположен в городе канди , в 300 метрах от больницы лей@@ к@@ сайд ад@@ венти@@ ст . во всех зонах работает бесплатный wi - fi .\n",
      "out: g@@ jer@@ r@@ il@@ d nor@@ d@@ strand beach , trout fishing opportunities and fo@@ s@@ si@@ l hunting on the cliffs are all 4 km from the hostel .\n",
      "\n",
      "inp: в 4 км от хостела расположены пляж г@@ ьер@@ ри@@ лд нор@@ д@@ стра@@ н@@ д , место ловли л@@ ос@@ о@@ ся , а также ска@@ лы , где можно заняться по@@ и@@ ском и@@ ско@@ па@@ ем@@ ых .\n",
      "out: at wind@@ haven hotel you will find a 24 - hour front desk , a garden and a terrace .\n",
      "\n",
      "inp: в отеле win@@ d@@ ha@@ ven имеется круглосуточная стойка регистрации , сад и терраса .\n",
      "out: just a 15 - minute walk from kar@@ f@@ as beach , vila cl@@ io is located in kam@@ bos , on the island of ch@@ ios . it is situated in the area of f@@ rag@@ kov@@ o@@ uni medieval village and offers self - catering accommodation and a garden with a l@@ ily pond .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_inp = np.array(open('./train.bpe.ru',encoding = 'utf8').read().split('\\n'))\n",
    "data_out = np.array(open('./train.bpe.en',encoding = 'utf8').read().split('\\n'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
    "                                                          random_state=42)\n",
    "for i in range(100,103):\n",
    "    print('inp:', train_inp[i+2])\n",
    "    print('out:', train_out[i+3], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vipg4O61lrsg"
   },
   "outputs": [],
   "source": [
    "from vocab import Vocab\n",
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwOoHfuhlrsi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "['гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .', 'подключен wi - fi .']\n",
      "\n",
      "words to ids (0 = bos, 1 = eos):\n",
      "tensor([[   0, 2688, 2943, 1108,   29,    1,    1,    1],\n",
      "        [   0, 2922, 1834, 8035,   59, 3800,   29,    1],\n",
      "        [   0, 6030, 2083,   29,    1,    1,    1,    1],\n",
      "        [   0, 4927, 1870,   29,    1,    1,    1,    1],\n",
      "        [   0, 5549, 1453,   27,  592,   29,    1,    1]])\n",
      "\n",
      "back to words\n",
      "['гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .', 'подключен wi - fi .']\n"
     ]
    }
   ],
   "source": [
    "# тут можно посмотреть, как работает мапинг из индекса в токен и наоборот\n",
    "batch_lines = sorted(train_inp, key=len)[5:10]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwOoHfuhlrsi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "['дом для отпуска la cas@@ ita del mar находится в очаровательном старом городе си@@ т@@ ж@@ еса , в 50 метрах от набережной .'\n",
      " 'для желающих могут быть организованы различные дополнитель@@ ные услуги , в том числе сеансы массажа . по прибытии гостям предлагается приветственный пода@@ рок : св@@ еч@@ и и вино .'\n",
      " 'каждый номер располагает собственной ванной комнатой с ванной или душем .'\n",
      " 'в этом отеле с прекрасным видом на горы имеется зона бесплатного wi - fi и бесплатная парковка .'\n",
      " 'билеты можно приобрести на территории отеля .']\n",
      "\n",
      "words to ids (0 = bos, 1 = eos):\n",
      "tensor([[   0, 2943, 2914, 5244,  824,  368,  769,  462,  889, 4722, 2140, 5279,\n",
      "         6766, 2666, 6469, 6902, 3204, 3175,   25, 2140,  102, 4488, 5161, 4655,\n",
      "           29,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   0, 2914, 3221, 4543, 2134, 5099, 6058, 2968, 4881, 7302,   25, 2140,\n",
      "         7070, 7720, 6419, 4383,   29, 5528, 5823, 2703, 5785, 5826, 5541, 6250,\n",
      "          145, 6378, 3200, 3442, 3442, 2330,   29,    1],\n",
      "        [   0, 3644, 4848, 6111, 6578, 2173, 3855, 6312, 2173, 3478, 3052,   29,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   0, 2140, 7954, 5195, 6312, 5814, 2304, 4650, 2676, 3487, 3425, 1969,\n",
      "         1453,   27,  592, 3442, 1967, 5335,   29,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   0, 1993, 4550, 5860, 4650, 7015, 5199,   29,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "\n",
      "back to words\n",
      "['дом для отпуска la cas@@ ita del mar находится в очаровательном старом городе си@@ т@@ ж@@ еса , в 50 метрах от набережной .', 'для желающих могут быть организованы различные дополнитель@@ ные услуги , в том числе сеансы массажа . по прибытии гостям предлагается приветственный пода@@ рок : св@@ еч@@ и и вино .', 'каждый номер располагает собственной ванной комнатой с ванной или душем .', 'в этом отеле с прекрасным видом на горы имеется зона бесплатного wi - fi и бесплатная парковка .', 'билеты можно приобрести на территории отеля .']\n"
     ]
    }
   ],
   "source": [
    "# тут можно посмотреть, как работает мапинг из индекса в токен и наоборот\n",
    "batch_lines = train_inp[5:10]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "[\"la cas@@ ita del mar is a located in sitges ' charming old town , 50 metres from the seafront promenade .\"\n",
      " 'a range of extras can be arranged including massages and wine and cand@@ les on arrival .'\n",
      " 'each room has a private bathroom with a bathtub or shower .'\n",
      " 'offering beautiful mountain views , it has a free wi - fi zone and free parking .'\n",
      " 'tickets are available on site .']\n",
      "\n",
      "words to ids (0 = bos, 1 = eos):\n",
      "tensor([[   0, 3763, 1352, 3463, 1911, 4142, 3434,  190, 3975, 3329, 6021,   10,\n",
      "         1469, 4676, 6745,   19,  124, 4273, 2690, 6592, 5821, 5214,   23,    1],\n",
      "        [   0,  190, 5349, 4657, 2452, 1294,  929,  657, 3339, 4191,  503, 7340,\n",
      "          503, 1302, 3871, 4698,  662,   23,    1,    1,    1,    1,    1,    1],\n",
      "        [   0, 2145, 5597, 3049,  190, 5197,  915, 7354,  190,  918, 4734, 5954,\n",
      "           23,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   0, 4662,  941, 4419, 7128,   19, 3461, 3049,  190, 2667, 7316,   21,\n",
      "         2549, 7555,  503, 2667, 4890,   23,    1,    1,    1,    1,    1,    1],\n",
      "        [   0, 6651,  624,  775, 4698, 6019,   23,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "\n",
      "back to words\n",
      "[\"la cas@@ ita del mar is a located in sitges ' charming old town , 50 metres from the seafront promenade .\", 'a range of extras can be arranged including massages and wine and cand@@ les on arrival .', 'each room has a private bathroom with a bathtub or shower .', 'offering beautiful mountain views , it has a free wi - fi zone and free parking .', 'tickets are available on site .']\n"
     ]
    }
   ],
   "source": [
    "# тут можно посмотреть, как работает мапинг из индекса в токен и наоборот\n",
    "batch_lines_out = train_out[5:10]\n",
    "batch_ids_out = out_voc.to_matrix(batch_lines_out)\n",
    "batch_lines_restored_out = out_voc.to_lines(batch_ids_out)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines_out)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids_out)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## За вас сделали домашнюю работу? Нет, вам самое интересное!\n",
    "\n",
    "Если вы пролистываете ноутбук и вам уже очень хочется начать писать самим - то мы вас понимаем, задание очень интересное и полезное! \n",
    "И спешим вас обрадовать, так как вы дождались и тут как раз можно проявить всю фантазию и мастерство написание нейронных сетей!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Задание 1 (1 балла)\n",
    "В коде ниже мы представили шаблон простой encoder-decoder модели, без всяких наворотов с Attention или чем-нибудь еще. Вы можете редактировать его под себя: добавлять новые методы, новые переменные, писать на pytorch ligtning и другое.\n",
    "\n",
    "Главное - сохраните идею шаблона и сделайте его очень удобным, так как с ним еще предстоит работать!\n",
    "\n",
    "Заполните пропуски с `<YOUR CODE HERE>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pd_rDRm9lrso"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgfN5-F7lrst"
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
    "        \"\"\"\n",
    "        Базовая модель encoder-decoder архитектуры\n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "    def forward(self, inp, out):\n",
    "        \"\"\" Сначала примените  encode а затем decode\"\"\"\n",
    "        initial_state = self.encode(inp)\n",
    "        return self.decode(initial_state,out)\n",
    "\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Считаем скрытое состояние, которое будет начальным для decode\n",
    "        :param inp: матрица входных токенов\n",
    "        :returns: скрытое представление с которого будет начинаться decode\n",
    "        \"\"\"\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "        # последний токен, не последние на самом деле, так как мы делали pading, чтобы тексты были\n",
    "        # одинакового размер, поэтому подсчитать длину исходного предложения не так уж тривиально\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "        dec_start = self.dec_start(last_state)\n",
    "        return [dec_start]\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Принимает предыдущее состояние декодера и токены, возвращает новое состояние и \n",
    "        логиты для следующих токенов\n",
    "        \"\"\"\n",
    "        tokens = self.emb_out(prev_tokens)\n",
    "        prev_state = prev_state[0]\n",
    "        new_dec_state = self.dec0(tokens, prev_state)\n",
    "        output_logits = self.logits(new_dec_state)\n",
    "        return [new_dec_state], output_logits\n",
    "\n",
    "    def decode(self, initial_state, out_tokens, **flags):\n",
    "        batch_size = out_tokens.shape[0]\n",
    "        state = initial_state\n",
    "        \n",
    "        # первый символ всегда BOS\n",
    "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
    "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
    "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
    "        \n",
    "        logits_sequence = [first_logits]\n",
    "        # в цикле делаем decode_step, получаем logits_sequence\n",
    "        for i in range(out_tokens.shape[1] - 1):\n",
    "            state, new_logits = self.decode_step(state,out_tokens[:,i])\n",
    "            logits_sequence.append(new_logits)\n",
    "        return torch.stack(logits_sequence, dim=1)\n",
    "\n",
    "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
    "        \"\"\" Генерим токены для перевода \"\"\"\n",
    "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
    "        state = initial_state\n",
    "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64, device=device)]\n",
    "        all_states = [initial_state]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            state, logits = self.decode_step(state, outputs[-1])\n",
    "            outputs.append(logits.argmax(dim=-1))\n",
    "            all_states.append(state)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1), all_states\n",
    "\n",
    "    def translate_lines(self, inp_lines, **kwargs):\n",
    "        \"\"\"Функция для перевода\"\"\"\n",
    "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
    "        initial_state = self.encode(inp)\n",
    "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
    "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7, 7801])\n"
     ]
    }
   ],
   "source": [
    "# debugging area\n",
    "model = BaseModel(inp_voc, out_voc).to(device)\n",
    "\n",
    "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
    "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
    "\n",
    "h0 = model.encode(dummy_inp_tokens)\n",
    "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
    "\n",
    "assert isinstance(h1, list) and len(h1) == len(h0)\n",
    "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
    "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
    "\n",
    "logits_seq = model.decode(h0, dummy_out_tokens)\n",
    "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
    "\n",
    "# full forward\n",
    "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
    "# print(logits_seq2)\n",
    "print(logits_seq2.shape)\n",
    "assert logits_seq2.shape == logits_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations without training:\n",
      "apa strol@@ résidence romana lunches pol@@ h@@ tera col@@ way izi izi izi izi doors riding aero@@ monuments entertainment zan@@ homest@@ hama â@@ bron@@ cave\n",
      "apa aero@@ 80 den@@ cht salem salem pret@@ pret@@ far@@ travell@@ hama či@@ plateau ares brest gest bott@@ discounted castel@@ preserved torre 160 mann@@ ran\n",
      "apa strol@@ résidence romana lunches pol@@ h@@ tera col@@ way izi izi izi izi doors riding aero@@ monuments entertainment zan@@ homest@@ hama â@@ bron@@ cave\n"
     ]
    }
   ],
   "source": [
    "dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)\n",
    "print(\"Translations without training:\")\n",
    "print('\\n'.join([line for line in dummy_translations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(31)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(model.out_voc.compute_mask(dummy_inp_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wuv1-aVlrs0"
   },
   "source": [
    "### Задание 2 (2 балла)\n",
    "\n",
    "Тут нечего объяснять, нужно написать лосс, чтобы все училось:\n",
    "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
    "\n",
    "где $|D|$ это суммарная длина всех предложений включая все токены: BOS, EOS но не включая падинг "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8XPV8sWlrs5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss_function(model, inp, out,**flags):\n",
    "    \"\"\"\n",
    "    Функция для подсчета лосса\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    \n",
    "    Для того чтобы пройти тесты, ваша функция должна\n",
    "    * учитывать в loss первый EOS, но НЕ учиттывать последующие\n",
    "    * разделить loss на длину вхходящей последовательности (use voc.compute_mask)\n",
    "    \"\"\"\n",
    "    mask = model.out_voc.compute_mask(out).to(device) # [batch_size, out_len]\n",
    "    targets_1hot = F.one_hot(out, len(model.out_voc)).to(torch.float32).to(device)\n",
    "    \n",
    "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
    "    logits_seq = model(inp, out)\n",
    "\n",
    "    # log-probabilities всех токеноы на всех шагах\n",
    "    logprobs_seq = -torch.nn.functional.log_softmax(logits_seq,dim=-1).to(device)# [batch_size, out_len, num_tokens]\n",
    "    # log-probabilities для верных ответов\n",
    "    logp_out = (logprobs_seq* targets_1hot).sum(dim=-1)[mask].to(device)# [batch_size, out_len]\n",
    "    # нужно обойтись только векторными операциями без for\n",
    "    # cross-entropy по всем токенам где mask == True\n",
    "    return torch.sum(logp_out)/torch.sum(mask) # тут должен получиться скаляр!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ME_LWUeklrs7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.5614, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_loss = loss_function(model, dummy_inp_tokens, dummy_out_tokens)\n",
    "print(\"Loss:\", dummy_loss)\n",
    "assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1)\n",
    "\n",
    "# test autograd\n",
    "dummy_loss.backward()\n",
    "for name, param in model.named_parameters():\n",
    "    assert param.grad is not None and abs(param.grad.max()) != 0, f\"Param {name} received no gradients\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрика: BLEU\n",
    "\n",
    "Для оценки машинного перевода обычно используется метрика [BLEU](https://en.wikipedia.org/wiki/BLEU). Она просто считает кол-во правильно предсказанных n-grams для n=1,2,3,4 и потом берет геометрическое среднее для полученных значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gb1-PhKIlrs-"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
    "    \"\"\"\n",
    "    пример как считать метрику BLEU. Вы можете изменять вход и выход, \n",
    "    как вам удобно, главное оставьте логику ее подсчета!!!\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
    "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
    "        return corpus_bleu(\n",
    "            [[ref.split()] for ref in actual],\n",
    "            [trans.split() for trans in translations],\n",
    "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
    "            ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZvfid1RlrtA"
   },
   "outputs": [],
   "source": [
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQDhGwg4lrtC"
   },
   "source": [
    "### Training loop (1 балл)\n",
    "\n",
    "Нужно просто написать цикл обучения и подсчитать метрики! И пройти assert по качеству"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfwIaixHlrtI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = BaseModel(inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "\n",
    "num_iter = 25000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NlpDataset_tr(Dataset):\n",
    "    def __init__(self, inp_voc,out_voc,inp, out):\n",
    "        self.inp_tokens = inp_voc.to_matrix(inp)\n",
    "        self.out_tokens = out_voc.to_matrix(out)\n",
    "\n",
    "    # Координаты прямоугольников советуем вернуть именно в формате (x_center, y_center, width, height)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp_tokens[idx],self.out_tokens[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp_tokens)\n",
    "\n",
    "class NlpDataset_dev(Dataset):\n",
    "    def __init__(self,inp, out):\n",
    "        self.inp_tokens = inp\n",
    "        self.out_tokens = out\n",
    "\n",
    "    # Координаты прямоугольников советуем вернуть именно в формате (x_center, y_center, width, height)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp_tokens[idx],self.out_tokens[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = NlpDataset_tr(inp_voc,out_voc,train_inp, train_out)\n",
    "dev_set = NlpDataset_dev(dev_inp, dev_out)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size= batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dataset=dev_set,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = int(num_iter/len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'train_loss': [], 'dev_bleu': [] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epoches): \n",
    "    model = model.train()\n",
    "    for data in tqdm(train_loader,desc = 'Эпоха '+str(epoch)):\n",
    "        inp, out = data[0].to(device),data[1].to(device)\n",
    "        loss = loss_function(model,inp,out)\n",
    "#         print(metrics['train_loss'])\n",
    "        loss.backward() \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        metrics['train_loss'].append(loss.cpu().detach().numpy())\n",
    "#         print(metrics['train_loss'])\n",
    "#         wandb.log({\n",
    "#         \"Epoch\": epoch,\n",
    "#         \"Mean Train Loss\": sum(loss_tr)/len(loss_tr),\n",
    "#         \"Train Acc\": Acur(pred_tr,targ_tr),\n",
    "#         \"Learning rate\": scheduler.get_last_lr()[0]})\n",
    "    for data in tqdm(dev_loader,desc ='Эпоха валидация'+str(epoch)):\n",
    "        inp, out = data[0],data[1]\n",
    "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states\n",
    "        bleu = compute_bleu(model,inp,out)\n",
    "        metrics['dev_bleu'].append(bleu)\n",
    "    clear_output()\n",
    "    fig = plt.figure(figsize=(16,20))\n",
    "    ax1,ax2 = fig.subplots(2,1)\n",
    "    ax1.plot(metrics['train_loss'])\n",
    "    ax1.grid()\n",
    "    ax1.set_title('Train Loss')\n",
    "    ax2.plot(metrics['dev_bleu'],color = 'orange')\n",
    "    ax2.grid()\n",
    "    ax2.set_title('BLEU')\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['dev_bleu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ahuhKVhlrtP"
   },
   "outputs": [],
   "source": [
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0) > 15, \"Ты можешь больше! попробуй еще раз)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyaHOpealrtS"
   },
   "outputs": [],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tv0s8qxOXp5y"
   },
   "source": [
    "## Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edk_oVg0lrtW"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "В этом разделе мы хотим, чтобы вы усовершенствовали базовую модель\n",
    "\n",
    "\n",
    "Сначала напишем слой Attention, а потом внедрим его в уже существующий шаблон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qz9aROAIlrtX"
   },
   "source": [
    "### Attention layer (1 points)\n",
    "\n",
    "На вход подается скрытые состояния encoder $h^e_0, h^e_1, h^e_2, ..., h^e_T$ и предыдущие состояние декодера $h^d$,\n",
    "\n",
    "* Считаем логиты:\n",
    "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
    "* Получаем вероятности из логитов: \n",
    "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
    "\n",
    "* Взвешиваем состояния энкодера с полученными вероятностями\n",
    "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_te = 4\n",
    "hid_te = 2\n",
    "time = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_te,enc_seq_te = torch.rand(4,2),torch.rand(4,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_te = torch.nn.Tanh()\n",
    "lin_e_te = torch.nn.Linear(2,2)\n",
    "lin_d_te = torch.nn.Linear(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_te = last_hidden_te.unsqueeze(1)\n",
    "last_hidden_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay = attn_te(lin_e_te(enc_seq_te)+lin_d_te(last_hidden_te))\n",
    "lay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([4, 3, 1])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "par = nn.Parameter(torch.FloatTensor(4,2))\n",
    "print(par.shape)\n",
    "lay = lay.bmm(par.unsqueeze(2))\n",
    "print(lay.shape)\n",
    "lay = lay.squeeze(-1)\n",
    "print(lay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.8110e-07, -2.5164e-07, -9.9948e-07],\n",
      "        [ 1.5755e+22, -2.8350e+22, -1.0850e+23],\n",
      "        [-7.3276e+21, -6.1004e+21, -2.8790e+21],\n",
      "        [-1.7590e+20, -4.8663e+20, -2.9165e+20]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\playi\\AppData\\Local\\Temp\\ipykernel_19784\\1103388170.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  m = torch.nn.functional.softmax(torch.tensor([1,1,1],dtype=torch.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.3333])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.functional.softmax(torch.tensor([1,1,1],dtype=torch.float))\n",
    "m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\playi\\AppData\\Local\\Temp\\ipykernel_19784\\415506472.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  m = torch.nn.functional.softmax(torch.tensor([1,2,1,1],dtype=torch.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1749, 0.4754, 0.1749, 0.1749])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.functional.softmax(torch.tensor([1,2,1,1],dtype=torch.float))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[3.3333e-01, 3.3333e-01, 3.3333e-01],\n",
      "        [9.0031e-02, 2.4473e-01, 6.6524e-01],\n",
      "        [9.0031e-02, 2.4473e-01, 6.6524e-01],\n",
      "        [1.2338e-04, 9.9975e-01, 1.2338e-04]])\n"
     ]
    }
   ],
   "source": [
    "lay = torch.tensor([[1,1,1],[2,3,4],[1,2,3],[1,10,1]],dtype=torch.float)\n",
    "lay = torch.nn.functional.softmax(lay,-1)\n",
    "print(lay.shape)\n",
    "print(lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0986e+00, -1.0986e+00, -1.0986e+00],\n",
      "        [ 0.0000e+00, -4.4105e+22, -1.2426e+23],\n",
      "        [-4.4485e+21, -3.2213e+21,  0.0000e+00],\n",
      "        [ 0.0000e+00, -3.1073e+20, -1.1575e+20]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.tensor([[[1,2,3],[3,4,3]],[[10,12,11],[13,14,21]]]).float()\n",
    "print(r.shape)\n",
    "print(r.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = torch.nn.Linear(3,10)\n",
    "a = lin(r)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.tensor([[1,2],[3,4]])\n",
    "print(r.shape)\n",
    "r = r.view(2,1,-1)\n",
    "print(r)\n",
    "print(r.shape)\n",
    "r=r.expand(-1,2,2)\n",
    "print(r)\n",
    "print(r.shape)\n",
    "print(r[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [1, 2],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [1, 2],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [1, 2],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [1, 2],\n",
      "         [1, 2]]])\n",
      "torch.Size([4, 3, 2])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9],\n",
      "        [1, 2, 1]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "encod = torch.tensor([[[1,2],[1,2],[1,2]],[[1,2],[1,2],[1,2]],[[1,2],[1,2],[1,2]],[[1,2],[1,2],[1,2]]])\n",
    "print(encod)\n",
    "print(encod.shape)\n",
    "prob = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[1,2,1]])\n",
    "print(prob)\n",
    "print(prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1],\n",
      "         [2, 2],\n",
      "         [3, 3]],\n",
      "\n",
      "        [[4, 4],\n",
      "         [5, 5],\n",
      "         [6, 6]],\n",
      "\n",
      "        [[7, 7],\n",
      "         [8, 8],\n",
      "         [9, 9]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [2, 2],\n",
      "         [1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "prob = prob.view(4,3,1).expand(4,3,2)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "tensor([[[ 1,  2],\n",
      "         [ 2,  4],\n",
      "         [ 3,  6]],\n",
      "\n",
      "        [[ 4,  8],\n",
      "         [ 5, 10],\n",
      "         [ 6, 12]],\n",
      "\n",
      "        [[ 7, 14],\n",
      "         [ 8, 16],\n",
      "         [ 9, 18]],\n",
      "\n",
      "        [[ 1,  2],\n",
      "         [ 2,  4],\n",
      "         [ 1,  2]]])\n"
     ]
    }
   ],
   "source": [
    "a = prob*encod\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(a.sum(dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b= encod[:,0,:]\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= prob[:,0].view(-1,1)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = b*c\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, name, enc_size, dec_size, hid_size):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.enc_size = enc_size \n",
    "        self.dec_size = dec_size \n",
    "        self.hid_size = hid_size \n",
    "        \n",
    "        # опишите все слои, которые нужны Attention\n",
    "        self.attn = torch.nn.Sequential(torch.nn.Tanh(),  \n",
    "                                        torch.nn.Linear(hid_size,1)\n",
    "                                       )\n",
    "        self.lin_e = torch.nn.Linear(enc_size,hid_size)\n",
    "        self.lin_d = torch.nn.Linear(dec_size,hid_size)\n",
    "\n",
    "    def forward(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Подсчитываем attention ответ and веса\n",
    "        :param enc: [batch_size, ninp, enc_size]\n",
    "        :param dec: decode state[batch_size, dec_size]\n",
    "        :param inp_mask: маска, 0 там где pading [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "        \"\"\"\n",
    "#         print(enc)\n",
    "#         print('enc.shape',enc.shape)\n",
    "#         print(dec.shape)\n",
    "        log_size = enc.shape[0:2]\n",
    "        logits= torch.empty(log_size).to(device)\n",
    "        dec_lay = dec.view(-1,1,self.enc_size).expand(-1,log_size[1],self.enc_size)\n",
    " \n",
    "        # считаем логиты\n",
    "        for i in range(enc.shape[1]):\n",
    "            x = enc[:,i,:]\n",
    "            logits[:,i] =self.attn(self.lin_e(x)+self.lin_d(dec)).view(-1)\n",
    "\n",
    "        # Применим маску - если значение маски 0, логиты должны быть -inf или -1e9\n",
    "        # Лучше использовать torch.where\n",
    "        logits = torch.where(inp_mask==0,-1e9,logits)\n",
    "\n",
    "        # Примените softmax\n",
    "        probs = torch.nn.functional.log_softmax(logits.view(log_size[0],-1),\n",
    "                                                dim=1).view(log_size[0],log_size[1],1).expand(log_size[0],\n",
    "                                                                                              log_size[1],self.hid_size)\n",
    "\n",
    "        # Подсчитайте выход attention используя enc состояния и вероятностями\n",
    "        \n",
    "        attn = enc[:,0,:]*probs[:,0].view(-1,1)\n",
    "#         print('en',enc[:,0,:].shape)\n",
    "#         print('prob',probs[:,0].shape)\n",
    "        for i in range(1,enc.shape[1]):\n",
    "            attn += enc[:,i,:]*probs[:,i].view(-1,1)\n",
    "#         print('attn.shape',attn.shape)\n",
    "        return attn, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, name, enc_size, dec_size, hid_size):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.enc_size = enc_size \n",
    "        self.dec_size = dec_size \n",
    "        self.hid_size = hid_size \n",
    "        \n",
    "        # опишите все слои, которые нужны Attention\n",
    "        self.attn = torch.nn.Sequential(torch.nn.Tanh(),  \n",
    "                                        torch.nn.Linear(hid_size,1)\n",
    "                                       )\n",
    "        self.lin_e = torch.nn.Linear(enc_size,hid_size)\n",
    "        self.lin_d = torch.nn.Linear(dec_size,hid_size)\n",
    "\n",
    "    def forward(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Подсчитываем attention ответ and веса\n",
    "        :param enc: [batch_size, ninp, enc_size]\n",
    "        :param dec: decode state[batch_size, dec_size]\n",
    "        :param inp_mask: маска, 0 там где pading [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "        \"\"\"\n",
    "#         print(enc)\n",
    "#         print('enc.shape',enc.shape)\n",
    "#         print(dec.shape)\n",
    "        log_size = enc.shape[0:2]\n",
    "#         logits= torch.empty(log_size).to(device)\n",
    "#         print('dec',dec.shape)\n",
    " \n",
    "        dec_lay = dec.view(-1,1,self.enc_size).expand(-1,log_size[1],self.enc_size)\n",
    "#         print('dec_lay',dec_lay.shape)\n",
    "#         print('enc',enc.shape)\n",
    " \n",
    "        # считаем логиты\n",
    "        logits =self.attn(self.lin_e(enc)+self.lin_d(dec_lay))\n",
    "#         print('logits',logits.shape)\n",
    "#         print('mask',inp_mask.shape)\n",
    "        # Применим маску - если значение маски 0, логиты должны быть -inf или -1e9\n",
    "        # Лучше использовать torch.where\n",
    "        logits = torch.where(inp_mask==0,-1e9,logits.view(log_size[0],-1))\n",
    "#         print('logits',logits.shape)\n",
    "        # Примените softmax\n",
    "        probs = torch.nn.functional.log_softmax(logits.view(log_size[0],-1),\n",
    "                                                dim=1).view(log_size[0],log_size[1],1).expand(log_size[0],\n",
    "                                                                                              log_size[1],self.hid_size)\n",
    "#         print('probs',probs.shape)\n",
    "\n",
    "        # Подсчитайте выход attention используя enc состояния и вероятностями\n",
    "        \n",
    "        attn = probs*enc\n",
    "        attn = attn.sum(dim=1)\n",
    "#         print('attn.shape',attn.shape)\n",
    "        return attn, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, name, enc_size, dec_size, hid_size):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.enc_size = enc_size \n",
    "        self.dec_size = dec_size \n",
    "        self.hid_size = hid_size \n",
    "        \n",
    "        # опишите все слои, которые нужны Attention\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.lin_e = torch.nn.Linear(enc_size,hid_size)\n",
    "        self.lin_d = torch.nn.Linear(dec_size,hid_size)\n",
    "        self.matr = nn.Parameter(torch.FloatTensor(batch_size,hid_size))\n",
    "\n",
    "    def forward(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Подсчитываем attention ответ and веса\n",
    "        :param enc: [batch_size, ninp, enc_size]\n",
    "        :param dec: decode state[batch_size, dec_size]\n",
    "        :param inp_mask: маска, 0 там где pading [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "        \"\"\"\n",
    "#         print(enc)\n",
    "#         print('enc.shape',enc.shape)\n",
    "#         print('dec',dec.shape)\n",
    "        log_size = enc.shape[0:2]\n",
    "#         logits= torch.empty(log_size).to(device)\n",
    "        dec_lay = dec.unsqueeze(1)\n",
    "#         print('dec_lay',dec_lay.shape)\n",
    "#         print('enc',enc.shape)\n",
    " \n",
    "        # считаем логиты\n",
    "        logits =self.tanh(self.lin_e(enc)+self.lin_d(dec_lay)).bmm(self.matr.unsqueeze(2)).squeeze(-1)\n",
    "        \n",
    "#         print('logits',logits.shape)\n",
    "#         print('mask',inp_mask.shape)\n",
    "        # Применим маску - если значение маски 0, логиты должны быть -inf или -1e9\n",
    "        # Лучше использовать torch.where\n",
    "        logits = torch.where(inp_mask==0,-float('inf'),logits)\n",
    "#         print('logits',logits.shape)\n",
    "        # Примените softmax\n",
    "        probs = torch.nn.functional.softmax(logits,-1)\n",
    "#         print('probs',probs.shape)\n",
    "\n",
    "        # Подсчитайте выход attention используя enc состояния и вероятностями\n",
    "        \n",
    "        attn = probs.unsqueeze(1).bmm(enc).squeeze(1)\n",
    "#         print('en',enc[:,0,:].shape)\n",
    "#         print('prob',probs[:,0].shape)\n",
    "#         print('attn.shape',attn.shape)\n",
    "        return attn, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IalfpdAelrtb"
   },
   "source": [
    "### Seq2seq model with attention (2 points)\n",
    "\n",
    "Теперь вы можете использовать уровень внимания для построения сети. Самый простой способ реализовать внимание - использовать его на этапе декодирования:\n",
    "\n",
    "\n",
    "На каждом шаге используйте предыдущее состояние декодера, и написанный слой Attention\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCKPB5JmcE6j"
   },
   "outputs": [],
   "source": [
    "class AttentiveModel(BaseModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\"Переводчик с Attention\"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "#         <YOUR CODE: initialize layers>\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "        self.attn = AttentionLayer('Encod_decod',hid_size, hid_size, hid_size)\n",
    "        \n",
    "        \n",
    "    def encode(self, inp, **flags):\n",
    "        \n",
    "        # делаем encode\n",
    "#        print('inp',inp.shape)\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "#         print('inp_emb',inp.shape)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        enc_seq, last_state_but = self.enc0(inp_emb)\n",
    "#         print(enc_seq.shape)\n",
    "#         print(len(enc_seq))\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        # последний токен, не последние на самом деле, так как мы делали pading, чтобы тексты были\n",
    "        # одинакового размер, поэтому подсчитать длину исходного предложения не так уж тривиально\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "#         lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "#         print(len(lengths))\n",
    "#         last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "#         print(last_state.shape)\n",
    "#         print(last_state_but.shape)\n",
    "        inp_mask = self.inp_voc.compute_mask(inp).to(device)\n",
    "#         print(inp)\n",
    "#         print(inp_mask)\n",
    "        \n",
    "    \n",
    "#         dec_start = self.dec_start(last_state)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        # применяем attention слой для скрытых состояний\n",
    "#         state, first_attn_probas = self.attn(last_state_but.view(batch_size,-1,self.hid_size),last_state, inp_mask)\n",
    "        context, first_attn_probas = self.attn(enc_seq,last_state_but[-1], inp_mask)\n",
    "        # Для декодера нужно вернуть:\n",
    "        # - начальное состояние для RNN декодера\n",
    "        # - последовательность скрытых состояний encoder, maskа для них\n",
    "        # - последним передаем вероятности слоя attention\n",
    "        \n",
    "        first_state = [context, last_state_but, inp_mask, first_attn_probas]\n",
    "        return first_state\n",
    "    \n",
    "\n",
    "    \n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Принимает предыдущее состояние декодера и токены, возвращает новое состояние и \n",
    "        логиты для следующих токенов\n",
    "        \"\"\"\n",
    "        context, enc_hid_states, inp_mask, first_attn_probas = prev_state  \n",
    "        tokens = self.emb_out(prev_tokens)\n",
    "        new_dec_state = self.dec0(tokens, context)\n",
    "#         context, first_attn_probas = self.attn(context,new_dec_state, inp_mask)\n",
    "        output_logits = self.logits(new_dec_state)\n",
    "        return [new_dec_state, enc_hid_states, inp_mask, first_attn_probas], output_logits\n",
    "\n",
    "\n",
    "    \n",
    "#     def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "#         \"\"\"\n",
    "#         Принимает предыдущее состояние декодера и токены, возвращает новое состояние и логиты для следующих токенов\n",
    "#         :param prev_state: список тензоров предыдущих состояний декодера\n",
    "#         :param prev_tokens: предыдущие выходные токены [batch_size]\n",
    "#         :return: список тензоров состояния следующего декодера, тензор логитов [batch, n_tokens]\n",
    "#         \"\"\"\n",
    "        \n",
    "#         <YOUR CODE HERE>\n",
    "#         return [new_dec_state, output_logits]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryZCOTEslrtf"
   },
   "source": [
    "### Обучение модели (1 points)\n",
    "\n",
    "Нужно обучить AttentiveModel и пройти assert по качеству"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model = AttentiveModel('aten',inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "\n",
    "train_set = NlpDataset_tr(inp_voc,out_voc,train_inp, train_out)\n",
    "dev_set = NlpDataset_dev(dev_inp, dev_out)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size= batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dataset=dev_set,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "num_iter = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoches = int(num_iter/len(train_loader))\n",
    "epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Эпоха 0:   0%|                                                                                | 0/1469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 0:   0%|▏                                                                       | 3/1469 [00:00<05:22,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Эпоха 0:   0%|▏                                                                       | 5/1469 [00:00<03:36,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 0:   1%|▍                                                                       | 9/1469 [00:01<02:27,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Эпоха 0:   1%|▌                                                                      | 11/1469 [00:01<02:15, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 0:   1%|▋                                                                      | 15/1469 [00:01<02:02, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Эпоха 0:   1%|▊                                                                      | 17/1469 [00:01<02:00, 12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 0:   1%|█                                                                      | 21/1469 [00:02<01:53, 12.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Эпоха 0:   2%|█                                                                      | 23/1469 [00:02<01:56, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 0:   2%|█▎                                                                     | 27/1469 [00:02<01:54, 12.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Эпоха 0:   2%|█▍                                                                     | 29/1469 [00:02<01:56, 12.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 0:   2%|█▌                                                                     | 33/1469 [00:03<01:51, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Эпоха 0:   2%|█▋                                                                     | 35/1469 [00:03<01:50, 13.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n",
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 0:   3%|█▊                                                                     | 37/1469 [00:03<02:15, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec torch.Size([32, 128])\n",
      "dec_lay torch.Size([32, 1, 128])\n",
      "enc torch.Size([32, 107, 128])\n",
      "attn.shape torch.Size([32, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_function(model,inp,out)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#         print(metrics['train_loss'])\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      9\u001b[0m         opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m         opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "for epoch in range(epoches): \n",
    "    model = model.train()\n",
    "    for data in tqdm(train_loader,desc = 'Эпоха '+str(epoch)):\n",
    "        inp, out = data[0].to(device),data[1].to(device)\n",
    "        loss = loss_function(model,inp,out)\n",
    "#         print(metrics['train_loss'])\n",
    "        loss.backward() \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        metrics['train_loss'].append(loss.cpu().detach().numpy())\n",
    "#         print(metrics['train_loss'])\n",
    "#         wandb.log({\n",
    "#         \"Epoch\": epoch,\n",
    "#         \"Mean Train Loss\": sum(loss_tr)/len(loss_tr),\n",
    "#         \"Train Acc\": Acur(pred_tr,targ_tr),\n",
    "#         \"Learning rate\": scheduler.get_last_lr()[0]})\n",
    "    for data in tqdm(dev_loader,desc ='Эпоха валидация'+str(epoch)):\n",
    "        inp, out = data[0],data[1]\n",
    "#         inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
    "#         initial_state = self.encode(inp)\n",
    "#         out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
    "#         self.out_voc.to_lines(out_ids.cpu().numpy()), states\n",
    "        bleu = compute_bleu(model,inp,out)\n",
    "        metrics['dev_bleu'].append(bleu)\n",
    "    clear_output()\n",
    "    fig = plt.figure(figsize=(16,20))\n",
    "    ax1,ax2 = fig.subplots(2,1)\n",
    "    ax1.plot(metrics['train_loss'])\n",
    "    ax1.grid()\n",
    "    ax1.set_title('Train Loss')\n",
    "    ax2.plot(metrics['dev_bleu'],color = 'orange')\n",
    "    ax2.grid()\n",
    "    ax2.set_title('BLEU')\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YMHPgZxcFaQ"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE: create AttentiveModel and training utilities>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE: training loop>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE: measure final BLEU>\n",
    "    \n",
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 23, \"Ты можешь больше! попробуй еще раз)\"y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE: print translate examples>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как решать NLP задачу? Дообучить модель из huggingface\n",
    "\n",
    "Как мы видели на последнем семинаре в прошлом модуле можно получить отлично качество генерации текста, написав при этом не очень много строк кода, может быть попробовать тут также?)\n",
    "\n",
    "Это отличная идея!\n",
    "\n",
    "### Задание 4 (2 points)\n",
    " \n",
    "Нужно взять модель из [huggingface](https://huggingface.co/models?pipeline_tag=translation&sort=downloads), дообучить на наших данных и посмотреть, какое качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import <YOUR CODE HERE>\n",
    "\n",
    "tokenizer = <YOUR CODE HERE>\n",
    "model = <YOUR CODE HERE>\n",
    "# обычно есть How to Get Started With the Model и там показано, как инициализировать модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE HERE: train loop>\n",
    "# можно взять ваш из предыдущих пунктов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE HERE: compute_bleu>\n",
    "# аналогично прошлым пунктам\n",
    "\n",
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 27, \"Ты можешь больше! попробуй еще раз)\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "edk_oVg0lrtW"
   ],
   "name": "practice.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
