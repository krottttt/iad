{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Домашняя работа\n",
    "\n",
    "В этой работе вам предстоит с помощью encoder-decoder архитектуры, пробуя различные ее реализации, решить задачу машинного перевода.\n",
    "\n",
    "#### Наша задача - сделать свой собственный переводчик!\n",
    "\n",
    "Пока что только русско-английский:) Будем учиться на текстах описания отелей, так что при успешном выполнении этого задания у вас не возникнет проблем с выбором места для остановки в путешествии, так как все отзывы вам будут высококлассно переведены!"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "HmsFABwClrsS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Что необходимо обсудить до начала работы?\n",
    " \n",
    "*как токенезовать и закодировать текст?*\n",
    "\n",
    "С токенезацией хорошо справится WordPunctTokenizer из библиотеки nltk, а вот с кодированием не все так просто, как может показаться... \n",
    "\n",
    "В наших текстах очень много редких и очень мало встречаемых слов (в каждом отеле есть своя фишка: какой-то предмет декорации или услуга, которая описывается своим словом, которое только там и встречается). Если мы будем кодировать все слова, то размер нашего словаря будет очень-очень большим.\n",
    "\n",
    "Но на одном из семинаров мы кодировали побуквенно, кажется, что тут это может помочь! Да, действительно так, но придется очень очень долго обучать модель, а путешествовать и выбрать хороший отель уже хочется, поэтому мы придем к чему-то среднему между этими подходами -  [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) известный как __BPE__\n",
    "\n",
    "Этот алгоритм стартует с посимвольного уровня и итеративно мерджит самые встречаемые пары. И так N итераций. На выходе мы получаем самые частые последовательности символов из которых формируются слова!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "BPE - очень популярный и частоиспользуемый алгоритм в задачах NLP, поэтому есть много открытых реализаций этого алгоритма\n",
    "\n",
    "Мы уверены, что вы научились гуглить и искать полезные материалы в интернете, когда делали домашнее задание по YOLO, поэтому в этот раз просто покажем один из способов, как это можно сделать и затем в своих проектах вы можете брать этот подход и, возможно, как-то улучшать его!\n",
    "\n",
    "Тем кому очень интересно, как же все работает - заходите в файл vocab.py, очень советуем!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "def tokenize(x):\n",
    "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
    "\n",
    "# разбиваем и токенизируем тексты, записываем обработанные токены в файл\n",
    "with open('train.en', 'w') as f_src,  open('train.ru', 'w') as f_dst:\n",
    "    for line in open('data.txt'):\n",
    "        src_line, dst_line = line.strip().split('\\t')\n",
    "        f_src.write(tokenize(src_line) + '\\n')\n",
    "        f_dst.write(tokenize(dst_line) + '\\n')\n",
    "\n",
    "# строим и применяем bpe кодирование\n",
    "bpe = {}\n",
    "for lang in ['en', 'ru']:\n",
    "    learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
    "    bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
    "    \n",
    "    with open('train.bpe.' + lang, 'w') as f_out:\n",
    "        for line in open('train.' + lang):\n",
    "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:03<00:00, 2360.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:03<00:00, 2127.89it/s]\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9kP0SdxlrsY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Построение словарей, разбиение данных\n",
    "\n",
    "Сейчас, когда мы обучили BPE алгоритм на наших данных, построим словарь соответствия токена и его индекса, чтобы нам было затем удобно смотреть переводы и переводить новые предложения\n",
    "\n",
    "Также сделаем разбиение на train/test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmTy_m_olrsb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data_inp = np.array(open('./train.bpe.ru').read().split('\\n'))\n",
    "data_out = np.array(open('./train.bpe.en').read().split('\\n'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
    "                                                          random_state=42)\n",
    "for i in range(3):\n",
    "    print('inp:', train_inp[i])\n",
    "    print('out:', train_out[i], end='\\n\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inp: на территории обустроена бесплатная частная парковка .\n",
      "out: free private parking is available on site .\n",
      "\n",
      "inp: кроме того , в 5 минутах ходьбы работают многочисленные бары и рестораны .\n",
      "out: guests can find many bars and restaurants within a 5 - minute walk .\n",
      "\n",
      "inp: отель san mi@@ gu@@ el расположен в центре мор@@ ели@@ и , в 750 метрах от главной площади города и кафедрального собора .\n",
      "out: hotel san miguel is located in central more@@ lia , 750 metres from the city ’ s main square and cathedral .\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PskgBSxlrsd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from vocab import Vocab\n",
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vipg4O61lrsg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# тут можно посмотреть, как работает мапинг из индекса в токен и наоборот\n",
    "batch_lines = sorted(train_inp, key=len)[5:10]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lines\n",
      "['гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .', 'подключен wi - fi .']\n",
      "\n",
      "words to ids (0 = bos, 1 = eos):\n",
      "tensor([[   0, 2688, 2943, 1108,   29,    1,    1,    1],\n",
      "        [   0, 2922, 1834, 8035,   59, 3800,   29,    1],\n",
      "        [   0, 6030, 2083,   29,    1,    1,    1,    1],\n",
      "        [   0, 4927, 1870,   29,    1,    1,    1,    1],\n",
      "        [   0, 5549, 1453,   27,  592,   29,    1,    1]])\n",
      "\n",
      "back to words\n",
      "['гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .', 'подключен wi - fi .']\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwOoHfuhlrsi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## За вас сделали домашнюю работу? Нет, вам самое интересное!\n",
    "\n",
    "Если вы пролистываете ноутбук и вам уже очень хочется начать писать самим - то мы вас понимаем, задание очень интересное и полезное! \n",
    "И спешим вас обрадовать, так как вы дождались и тут как раз можно проявить всю фантазию и мастерство написание нейронных сетей!\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Задание 1 (1 балла)\n",
    "В коде ниже мы представили шаблон простой encoder-decoder модели, без всяких наворотов с Attention или чем-нибудь еще. Вы можете редактировать его под себя: добавлять новые методы, новые переменные, писать на pytorch ligtning и другое.\n",
    "\n",
    "Главное - сохраните идею шаблона и сделайте его очень удобным, так как с ним еще предстоит работать!\n",
    "\n",
    "Заполните пропуски с `<YOUR CODE HERE>`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pd_rDRm9lrso"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
    "        \"\"\"\n",
    "        Базовая модель encoder-decoder архитектуры\n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "    def forward(self, inp, out):\n",
    "        \"\"\" Сначала примените  encode а затем decode\"\"\"\n",
    "        initial_state = <YOUR CODE HERE>\n",
    "        return <YOUR CODE HERE>\n",
    "\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Считаем скрытое состояние, которое будет начальным для decode\n",
    "        :param inp: матрица входных токенов\n",
    "        :returns: скрытое представление с которого будет начинаться decode\n",
    "        \"\"\"\n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "        # последний токен, не последние на самом деле, так как мы делали pading, чтобы тексты были\n",
    "        # одинакового размер, поэтому подсчитать длину исходного предложения не так уж тривиально\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "        dec_start = self.dec_start(last_state)\n",
    "        return [dec_start]\n",
    "\n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Принимает предыдущее состояние декодера и токены, возвращает новое состояние и \n",
    "        логиты для следующих токенов\n",
    "        \"\"\"\n",
    "        prev_gru0_state = prev_state[0]\n",
    "        \n",
    "        <YOUR CODE HERE>\n",
    "        \n",
    "        return new_dec_state, output_logits\n",
    "\n",
    "    def decode(self, initial_state, out_tokens, **flags):\n",
    "        batch_size = out_tokens.shape[0]\n",
    "        state = initial_state\n",
    "        \n",
    "        # первый символ всегда BOS\n",
    "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
    "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
    "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
    "        \n",
    "        logits_sequence = [first_logits]\n",
    "        # в цикле делаем decode_step, получаем logits_sequence\n",
    "        for i in range(out_tokens.shape[1] - 1):\n",
    "            <YOUR CODE HERE>\n",
    "        return torch.stack(logits_sequence, dim=1)\n",
    "\n",
    "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
    "        \"\"\" Генерим токены для перевода \"\"\"\n",
    "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
    "        state = initial_state\n",
    "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64, device=device)]\n",
    "        all_states = [initial_state]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            state, logits = self.decode_step(state, outputs[-1])\n",
    "            outputs.append(logits.argmax(dim=-1))\n",
    "            all_states.append(state)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1), all_states\n",
    "\n",
    "    def translate_lines(self, inp_lines, **kwargs):\n",
    "        \"\"\"Функция для перевода\"\"\"\n",
    "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
    "        initial_state = self.encode(inp)\n",
    "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
    "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgfN5-F7lrst"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# debugging area\n",
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "\n",
    "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
    "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
    "\n",
    "h0 = model.encode(dummy_inp_tokens)\n",
    "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
    "\n",
    "assert isinstance(h1, list) and len(h1) == len(h0)\n",
    "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
    "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
    "\n",
    "logits_seq = model.decode(h0, dummy_out_tokens)\n",
    "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
    "\n",
    "# full forward\n",
    "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
    "assert logits_seq2.shape == logits_seq.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)\n",
    "print(\"Translations without training:\")\n",
    "print('\\n'.join([line for line in dummy_translations]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание 2 (2 балла)\n",
    "\n",
    "Тут нечего объяснять, нужно написать лосс, чтобы все училось:\n",
    "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
    "\n",
    "где $|D|$ это суммарная длина всех предложений включая все токены: BOS, EOS но не включая падинг "
   ],
   "metadata": {
    "colab_type": "text",
    "id": "_wuv1-aVlrs0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def loss_function(model, inp, out, **flags):\n",
    "    \"\"\"\n",
    "    Функция для подсчета лосса\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    \n",
    "    Для того чтобы пройти тесты, ваша функция должна\n",
    "    * учитывать в loss первый EOS, но НЕ учиттывать последующие\n",
    "    * разделить loss на длину вхходящей последовательности (use voc.compute_mask)\n",
    "    \"\"\"\n",
    "    mask = model.out_voc.compute_mask(out) # [batch_size, out_len]\n",
    "    targets_1hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)\n",
    "    \n",
    "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
    "    logits_seq = <YOUR CODE HERE>\n",
    "\n",
    "    # log-probabilities всех токеноы на всех шагах\n",
    "    logprobs_seq = <YOUR CODE HERE> # [batch_size, out_len, num_tokens]\n",
    "   \n",
    "    # log-probabilities для верных ответов\n",
    "    logp_out = (logprobs_seq * targets_1hot).sum(dim=-1) # [batch_size, out_len]\n",
    "    # нужно обойтись только векторными операциями без for\n",
    "\n",
    "    # cross-entropy по всем токенам где mask == True\n",
    "    return <YOUR CODE HERE> # тут должен получиться скаляр!"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8XPV8sWlrs5",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)\n",
    "print(\"Loss:\", dummy_loss)\n",
    "assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1)\n",
    "\n",
    "# test autograd\n",
    "dummy_loss.backward()\n",
    "for name, param in model.named_parameters():\n",
    "    assert param.grad is not None and abs(param.grad.max()) != 0, f\"Param {name} received no gradients\""
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ME_LWUeklrs7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Метрика: BLEU\n",
    "\n",
    "Для оценки машинного перевода обычно используется метрика [BLEU](https://en.wikipedia.org/wiki/BLEU). Она просто считает кол-во правильно предсказанных n-grams для n=1,2,3,4 и потом берет геометрическое среднее для полученных значений."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
    "    \"\"\"\n",
    "    пример как считать метрику BLEU. Вы можете изменять вход и выход, \n",
    "    как вам удобно, главное оставьте логику ее подсчета!!!\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
    "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
    "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
    "        return corpus_bleu(\n",
    "            [[ref.split()] for ref in actual],\n",
    "            [trans.split() for trans in translations],\n",
    "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
    "            ) * 100"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gb1-PhKIlrs-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compute_bleu(model, dev_inp, dev_out)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZvfid1RlrtA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loop (1 балл)\n",
    "\n",
    "Нужно просто написать цикл обучения и подсчитать метрики! И пройти assert по качеству"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "nQDhGwg4lrtC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = BaseModel(inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "\n",
    "num_iter = 25000\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfwIaixHlrtI",
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 15, \"Ты можешь больше! попробуй еще раз)\""
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ahuhKVhlrtP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyaHOpealrtS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention is all you need"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Tv0s8qxOXp5y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание 3\n",
    "\n",
    "В этом разделе мы хотим, чтобы вы усовершенствовали базовую модель\n",
    "\n",
    "\n",
    "Сначала напишем слой Attention, а потом внедрим его в уже существующий шаблон"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "edk_oVg0lrtW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention layer (1 points)\n",
    "\n",
    "На вход подается скрытые состояния encoder $h^e_0, h^e_1, h^e_2, ..., h^e_T$ и предыдущие состояние декодера $h^d$,\n",
    "\n",
    "* Считаем логиты:\n",
    "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
    "* Получаем вероятности из логитов: \n",
    "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
    "\n",
    "* Взвешиваем состояния энкодера с полученными вероятностями\n",
    "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Qz9aROAIlrtX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, name, enc_size, dec_size, hid_size):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.enc_size = enc_size \n",
    "        self.dec_size = dec_size \n",
    "        self.hid_size = hid_size \n",
    "        \n",
    "        # опишите все слои, которые нужны Attention\n",
    "        \n",
    "\n",
    "    def forward(self, enc, dec, inp_mask):\n",
    "        \"\"\"\n",
    "        Подсчитываем attention ответ and веса\n",
    "        :param enc: [batch_size, ninp, enc_size]\n",
    "        :param dec: decode state[batch_size, dec_size]\n",
    "        :param inp_mask: маска, 0 там где pading [batch_size, ninp]\n",
    "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
    "        \"\"\"\n",
    "\n",
    "        # считаем логиты\n",
    "        <...>\n",
    "\n",
    "        # Применим маску - если значение маски 0, логиты должны быть -inf или -1e9\n",
    "        # Лучше использовать torch.where\n",
    "        <...>\n",
    "\n",
    "        # Примените softmax\n",
    "        probs = <...>\n",
    "\n",
    "        # Подсчитайте выход attention используя enc состояния и вероятностями\n",
    "        attn = <...>\n",
    "\n",
    "        return attn, probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seq2seq model with attention (2 points)\n",
    "\n",
    "Теперь вы можете использовать уровень внимания для построения сети. Самый простой способ реализовать внимание - использовать его на этапе декодирования:\n",
    "\n",
    "\n",
    "На каждом шаге используйте предыдущее состояние декодера, и написанный слой Attention\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "IalfpdAelrtb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AttentiveModel(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\"Переводчик с Attention\"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        <YOUR CODE: initialize layers>\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Считаем скрытые скрытые состояния, которые используем в decode\n",
    "        :param inp: матрица входных токенов\n",
    "        \"\"\"\n",
    "        \n",
    "        # делаем encode\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        # применяем attention слой для скрытых состояний\n",
    "        first_attn_probas = <...>\n",
    "        \n",
    "        # Для декодера нужно вернуть:\n",
    "        # - начальное состояние для RNN декодера\n",
    "        # - последовательность скрытых состояний encoder, maskа для них\n",
    "        # - последним передаем вероятности слоя attention\n",
    "        \n",
    "        first_state = [<...>, first_attn_probas]\n",
    "        return first_state\n",
    "   \n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Принимает предыдущее состояние декодера и токены, возвращает новое состояние и логиты для следующих токенов\n",
    "        :param prev_state: список тензоров предыдущих состояний декодера\n",
    "        :param prev_tokens: предыдущие выходные токены [batch_size]\n",
    "        :return: список тензоров состояния следующего декодера, тензор логитов [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        \n",
    "        <YOUR CODE HERE>\n",
    "        return [new_dec_state, output_logits]\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCKPB5JmcE6j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обучение модели (1 points)\n",
    "\n",
    "Нужно обучить AttentiveModel и пройти assert по качеству"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "ryZCOTEslrtf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "<YOUR CODE: create AttentiveModel and training utilities>"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YMHPgZxcFaQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "<YOUR CODE: training loop>"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "<YOUR CODE: measure final BLEU>\n",
    "    \n",
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 23, \"Ты можешь больше! попробуй еще раз)\"y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "<YOUR CODE: print translate examples>"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Как решать NLP задачу? Дообучить модель из huggingface\n",
    "\n",
    "Как мы видели на последнем семинаре в прошлом модуле можно получить отлично качество генерации текста, написав при этом не очень много строк кода, может быть попробовать тут также?)\n",
    "\n",
    "Это отличная идея!\n",
    "\n",
    "### Задание 4 (2 points)\n",
    " \n",
    "Нужно взять модель из [huggingface](https://huggingface.co/models?pipeline_tag=translation&sort=downloads), дообучить на наших данных и посмотреть, какое качество"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import <YOUR CODE HERE>\n",
    "\n",
    "tokenizer = <YOUR CODE HERE>\n",
    "model = <YOUR CODE HERE>\n",
    "# обычно есть How to Get Started With the Model и там показано, как инициализировать модель"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "<YOUR CODE HERE: train loop>\n",
    "# можно взять ваш из предыдущих пунктов"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "<YOUR CODE HERE: compute_bleu>\n",
    "# аналогично прошлым пунктам\n",
    "\n",
    "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 27, \"Ты можешь больше! попробуй еще раз)\""
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "edk_oVg0lrtW"
   ],
   "name": "practice.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}