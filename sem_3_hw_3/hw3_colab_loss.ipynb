{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKdcvMtSMpSg"
      },
      "source": [
        "#Майнор ИАД. Домашнее задание 3. YOLO.\n",
        "\n",
        "В этом задании вы напишете и обучите свой собственный YOLO детектор. Нужно будет разобраться со статьей: понять какого формата должна быть обучающая пара (x, y), как перевести лосс из математической формулы в питоновский код - ну и конечно понять и реализовать саму архитектуру модели.\n",
        "\n",
        "Выборка на котрой мы будем обучать модель состоит из разнообразных фотографий яблок, бананов и апельсинов. Данные скачиваем [отсюда](https://drive.google.com/file/d/1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3/view?usp=sharing).\n",
        "\n",
        "Баллы за ДЗ распределены следующим образом: \n",
        "- Выборка для YoloV1 - 2 балла\n",
        "- YOLO модель - 2 балла\n",
        "- YOLO Loss - 3 балла\n",
        "- Вспомогательные функции - 2 балла\n",
        "- Обучение и расчет метрик - 2 балла\n",
        "\n",
        "Для построения и обучения можно использовать как pytorch, так и pytorch-lightning.\n",
        "\n",
        "Да-да, баллов в сумме получается 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY4niK1xMpSg"
      },
      "outputs": [],
      "source": [
        "# Данная библиотека понадобится нам, чтобы обработать разметку\n",
        "! pip install xmltodict pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNSQ7FNss30F"
      },
      "source": [
        "Скачаем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN1dE6eY7PjV",
        "outputId": "e48e54df-c61e-4935-a446-6ad9cdc60438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "drwx------ 4 root root 4096 Nov 26 08:32 data\n",
            "drwxr-xr-x 3 root root 4096 Dec 10 19:25 __MACOSX\n",
            "drwxr-xr-x 1 root root 4096 Dec  8 14:36 sample_data\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3\" -O data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -q data.zip\n",
        "!rm data.zip\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep38vdW_s-Rz"
      },
      "source": [
        "Посмотрим как выглядит один из файлов разметки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqwaHl3ntBaN"
      },
      "outputs": [],
      "source": [
        "!cat data/train/apple_3.xml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdQxrA5_MpSg"
      },
      "source": [
        "## Релизуйте выборку для YoloV1 - 2 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXG9reop-BkS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "import tqdm\n",
        "import xmltodict\n",
        "\n",
        "from IPython.core.display import struct\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import albumentations as A\n",
        "import albumentations.pytorch\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import auc\n",
        "# Добавьте необходимые вам библиотеки, если их не окажется в списке выше"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = glob.glob(os.path.join('data/train/','*.jpg'))"
      ],
      "metadata": {
        "id": "iDnmmrDnII6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fileptr = open(\"/content/data/train/orange_76.xml\",\"r\")\n",
        "\n",
        "#read xml content from the file\n",
        "xml_content= fileptr.read()\n",
        "\n",
        "#change xml format to ordered dict\n",
        "my_ordered_dict=xmltodict.parse(xml_content)\n",
        "my_ordered_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6KKk6FORRzw",
        "outputId": "1c533858-ac36-4e5f-c4f0-5c58389ccfbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'annotation': {'folder': 'train',\n",
              "  'filename': 'orange_76.jpg',\n",
              "  'path': 'C:\\\\tensorflow1\\\\models\\\\research\\\\object_detection\\\\images\\\\train\\\\orange_76.jpg',\n",
              "  'source': {'database': 'Unknown'},\n",
              "  'size': {'width': '1620', 'height': '1080', 'depth': '3'},\n",
              "  'segmented': '0',\n",
              "  'object': [{'name': 'orange',\n",
              "    'pose': 'Unspecified',\n",
              "    'truncated': '1',\n",
              "    'difficult': '0',\n",
              "    'bndbox': {'xmin': '494', 'ymin': '1', 'xmax': '1452', 'ymax': '741'}},\n",
              "   {'name': 'orange',\n",
              "    'pose': 'Unspecified',\n",
              "    'truncated': '1',\n",
              "    'difficult': '0',\n",
              "    'bndbox': {'xmin': '822',\n",
              "     'ymin': '643',\n",
              "     'xmax': '1620',\n",
              "     'ymax': '1066'}}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(my_ordered_dict['annotation']['object'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42HlGET9SDlI",
        "outputId": "6c80a5c4-aa73-4f29-d4df-92a1ea555e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_t = xmltodict.parse(open(\"/content/data/train/orange_76.xml\",\"r\").read())['annotation']['object']\n",
        "dic_t[0]['bndbox']['xmax']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-KxcEtLuUDdp",
        "outputId": "8aae0f84-f8ba-471a-ba52-3b3e54142dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1452'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im = np.array(Image.open('/content/data/train/mixed_8.jpg').convert(\"RGB\"))\n",
        "im.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm1jlm4nVsrQ",
        "outputId": "8d9610d6-7fad-4bff-d4f9-33c75345ded1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "757"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gL8_CyyTYJ-"
      },
      "source": [
        "Так как в этом домашнем задании использовать аугментации для обучения __обязательно__ - советуем воспользоваться библиотекой albumentations.\n",
        "\n",
        "Она  особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Для знакомства с этим механизмом советуем следующий [гайд](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). \n",
        "\n",
        "Вы все еще можете избрать путь torchvision.transforms, вам потребуется знакомый нам метод `__getitem__`, однако вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную\n",
        "\n",
        "__Обратите внимание__ на то, что в статье коробки предсказаний параметризуются через: _(x_center, y_center, width, height)_ (причем эти значения _относительные_), а в наших файлах - это _(x_min, y_min, x_max, y_max)_\n",
        "\n",
        "Также, помните что модель должна предсказывать как прямоугольник с обьектом, так и вероятности каждого класса!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "V18tsTZLx-5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjZkU0vzMpSh"
      },
      "outputs": [],
      "source": [
        "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
        "\n",
        "class FruitDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.image_paths = glob.glob( os.path.join(data_dir,'*.jpg'))\n",
        "        self.enc = preprocessing.LabelEncoder()\n",
        "        self.box_paths = [x+\".xml\" for x in [os.path.splitext(x)[0] for x in self.image_paths]]\n",
        "        assert len(self.image_paths) == len(self.box_paths)\n",
        "        self.transforms = transforms\n",
        "        for idx in range(len(self.box_paths)):\n",
        "            image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
        "            _,classes =  self.__get_boxes_from_xml(self.box_paths[idx],image)\n",
        "            if idx == 0:\n",
        "              all_classes = classes\n",
        "            classes = np.array(classes)\n",
        "            all_classes = np.concatenate((all_classes,classes))\n",
        "        all_classes = np.unique(all_classes)\n",
        "        self.enc.fit(all_classes)\n",
        "\n",
        "\n",
        "    # Координаты прямоугольников советуем вернуть именно в формате (x_center, y_center, width, height)\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
        "        boxes, class_labels = self.__get_boxes_from_xml(self.box_paths[idx],image)\n",
        "        if self.transforms is not None:\n",
        "          augs = self.transforms(image=image, bboxes = boxes, class_labels=class_labels)\n",
        "          image = augs['image']\n",
        "          boxes = augs['bboxes']\n",
        "          class_labels = augs['class_labels']\n",
        "        boxes, class_labels = torch.as_tensor(boxes, dtype=torch.float64),  torch.as_tensor(self.enc.transform(class_labels),dtype=torch.int8)\n",
        "        return image.unsqueeze(dim=0), (boxes, class_labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __get_boxes_from_xml(self, xml_filename: str,image):\n",
        "      \"\"\"\n",
        "      Метод, который считает и распарсит (с помощью xmltodict) переданный xml\n",
        "      файл и вернет координаты прямоугольников обьектов на соответсвующей фотографии\n",
        "      и название класса обьекта в каждом прямоугольнике\n",
        "\n",
        "      Обратите внимание, что обьектов может быть как несколько, так и один единственный\n",
        "      \"\"\"\n",
        "      boxes = []\n",
        "      class_labels = []\n",
        "      dic = xmltodict.parse(open(xml_filename,\"r\").read())['annotation']\n",
        "      im_w = int(dic['size']['width'])\n",
        "      if im_w == 0:\n",
        "        im_w = image.shape[1]\n",
        "      im_h = int(dic['size']['height'])\n",
        "      if im_h == 0:\n",
        "        im_h = image.shape[0]\n",
        "      obj = dic['object']\n",
        "      if isinstance(obj, dict):\n",
        "        obj = [obj]\n",
        "      for item in obj:\n",
        "        boxes.append(self.__convert_to_yolo_box_params(\n",
        "            [int(x) for x in [item['bndbox']['xmin'], item['bndbox']['ymin'], item['bndbox']['xmax'], item['bndbox']['ymax']]],\n",
        "            im_w,im_h)\n",
        "        ) \n",
        "        class_labels.append(item['name'])      \n",
        "      return boxes,class_labels\n",
        "\n",
        "    def __convert_to_yolo_box_params(self, box_coordinates: List[int], im_w, im_h):\n",
        "      \"\"\"\n",
        "      Перейти от [xmin, ymin, xmax, ymax] к [x_center, y_center, width, height].\n",
        "      \n",
        "      Обратите внимание, что параметры [x_center, y_center, width, height] - это\n",
        "      относительные значение в отрезке [0, 1]\n",
        "\n",
        "      :param: box_coordinates - координаты коробки в формате [xmin, ymin, xmax, ymax]\n",
        "      :param: im_w - ширина исходного изображения\n",
        "      :param: im_h - высота исходного изображения\n",
        "\n",
        "      :return: координаты коробки в формате [x_center, y_center, width, height]\n",
        "      \"\"\"\n",
        "      ans = [] \n",
        "      ans.append((box_coordinates[0] + box_coordinates[2]) / 2 / im_w)# x_center  \n",
        "      ans.append((box_coordinates[1] + box_coordinates[3]) / 2 / im_h)  # y_center\n",
        "      ans.append((box_coordinates[2] - box_coordinates[0]) / im_w)# width  \n",
        "      ans.append((box_coordinates[3] - box_coordinates[1]) / im_h)  # height \n",
        "      return ans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([A.ToFloat (max_value=None, always_apply=False, p=1.0),\n",
        "    A.pytorch.transforms.ToTensorV2()\n",
        "                       ], \n",
        "                             bbox_params=A.BboxParams(format='yolo',\n",
        "                                                      label_fields=['class_labels']))\n",
        "train_dataset = FruitDataset(transforms=transform,\n",
        "    data_dir=\"./data/train\")\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=1)\n",
        "test_dataset = FruitDataset(transforms=transform,\n",
        "    data_dir=\"./data/test\")\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "PjfM6pjpH9qm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a2a9ce-583e-4915-821e-3c7d119b54b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_and_std(dataloader):\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "    for data, _ in dataloader:\n",
        "        # Mean over batch, height and width, but not over the channels\n",
        "        print(data)\n",
        "        channels_sum += torch.mean(data[0], dim=[0,2,3])\n",
        "        channels_squared_sum += torch.mean(data[0]**2, dim=[0,2,3])\n",
        "        num_batches += 1\n",
        "    \n",
        "    mean = channels_sum / num_batches\n",
        "\n",
        "    # std = sqrt(E[X^2] - (E[X])^2)\n",
        "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
        "\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "NCm-QYa_IAkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = get_mean_and_std(train_dataloader)\n",
        "mean_tr,std_tr = mean.tolist(),std.tolist()\n",
        "mean, std = get_mean_and_std(test_dataloader)\n",
        "mean_test,std_test = mean.tolist(),std.tolist()"
      ],
      "metadata": {
        "id": "P4w0K7-DIBwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmvn8pI5XPYh",
        "outputId": "83d9fea0-c751-4747-f80a-aa6713241539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8134564757347107, 0.6914446949958801, 0.5386325716972351]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "std_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET1UEyc4XRyn",
        "outputId": "b6280599-20a1-4b03-e480-741fdcb4e808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.27581149339675903, 0.32317855954170227, 0.40659812092781067]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGM42OVNYtmy",
        "outputId": "b74e087d-33d5-41df-c204-77b5a3a803fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8366211652755737, 0.7004529237747192, 0.56198650598526]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "std_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCtTASZxYu6t",
        "outputId": "b6959f94-abc0-447b-ba21-9489883e7b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.25586554408073425, 0.3319564163684845, 0.4067434072494507]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwXeSiAjdGeq"
      },
      "outputs": [],
      "source": [
        "WIDTH, HEIGHT = 448,448\n",
        "\n",
        "train_transform = A.Compose([A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
        "    A.GaussNoise (var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n",
        "    A.Normalize (mean=mean_tr, std=std_tr, max_pixel_value=255.0, always_apply=False, p=1.0),A.Resize(WIDTH, HEIGHT),\n",
        "    A.pytorch.transforms.ToTensorV2(), ], \n",
        "                             bbox_params=A.BboxParams(format='yolo',\n",
        "                                                      label_fields=['class_labels']))\n",
        "test_transform = A.Compose([A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
        "    A.GaussNoise (var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n",
        "    A.Normalize (mean=mean_test, std=std_test, max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "    A.pytorch.transforms.ToTensorV2()],\n",
        "                            bbox_params=A.BboxParams(format='yolo',\n",
        "                                                     label_fields=['class_labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayPwbRKocdCE",
        "outputId": "9c06feff-a7c3-4d5d-ceb8-9def67db9c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тесты успешно пройдены\n"
          ]
        }
      ],
      "source": [
        "train_dataset = FruitDataset(\n",
        "    transforms=train_transform,\n",
        "    data_dir=\"./data/train\"\n",
        "    )\n",
        "\n",
        "val_dataset = FruitDataset(\n",
        "    transforms=test_transform, \n",
        "    data_dir=\"./data/test\"\n",
        "    )\n",
        "\n",
        "# Немного проверок, чтобы убедиться в правильности направления решения\n",
        "assert isinstance(train_dataset[2], tuple)\n",
        "assert len(train_dataset[2]) == 2\n",
        "assert isinstance(train_dataset[2][0], torch.Tensor)\n",
        "assert isinstance(train_dataset[2][1], tuple)\n",
        "assert len(train_dataset[2][1]) == 2\n",
        "print(\"Тесты успешно пройдены\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "CZiK-5qz-eZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V1Tl_GAdeIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca74a32-208c-4721-ef43-4d9f2062e32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size= 16,\n",
        "    num_workers = 4,\n",
        "    shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=16,\n",
        "    num_workers = 4, \n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fRR9ns6MpSh"
      },
      "source": [
        "Теперь определим функцию для рассчета Intersection Over Union по 4 углам двух прямоугольников"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd88hnZiMpSh"
      },
      "outputs": [],
      "source": [
        "def intersection_over_union(predicted_bbox, gt_bbox) -> float:\n",
        "    \"\"\"\n",
        "    Intersection Over Union для двух прямоугольников\n",
        "\n",
        "    :param: predicted_bbox - [x_min, y_min, x_max, y_max]\n",
        "    :param: gt_bbox - [x_min, y_min, x_max, y_max]\n",
        "    \n",
        "    :return: Intersection Over Union\n",
        "    \"\"\"\n",
        "\n",
        "    intersection_bbox = np.array(\n",
        "        [\n",
        "            max(predicted_bbox[0], gt_bbox[0]),\n",
        "            max(predicted_bbox[1], gt_bbox[1]),\n",
        "            min(predicted_bbox[2], gt_bbox[2]),\n",
        "            min(predicted_bbox[3], gt_bbox[3]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
        "        intersection_bbox[3] - intersection_bbox[1], 0\n",
        "    )\n",
        "    area_dt = (predicted_bbox[2] - predicted_bbox[0]) * (predicted_bbox[3] - predicted_bbox[1])\n",
        "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
        "\n",
        "    union_area = area_dt + area_gt - intersection_area\n",
        "\n",
        "    iou = intersection_area / union_area\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = next(iter(train_dataloader))\n",
        "print(x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4AzwdwRCtb6",
        "outputId": "ea7f1098-c922-496b-e95d-20852f9fbba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((tensor([[0.2542, 0.4064, 0.3500, 0.5920],\n",
            "        [0.6193, 0.3504, 0.3427, 0.5152]], dtype=torch.float64), tensor([2, 2], dtype=torch.int8)), (tensor([[0.5166, 0.4232, 0.3047, 0.3750],\n",
            "        [0.5054, 0.5612, 0.8057, 0.2552],\n",
            "        [0.5239, 0.7240, 0.8232, 0.2708]], dtype=torch.float64), tensor([2, 1, 1], dtype=torch.int8)), (tensor([[0.4725, 0.5820, 0.9283, 0.4680]], dtype=torch.float64), tensor([1], dtype=torch.int8)), (tensor([[0.4319, 0.7062, 0.6722, 0.5877],\n",
            "        [0.7438, 0.4549, 0.5097, 0.5840]], dtype=torch.float64), tensor([0, 0], dtype=torch.int8)), (tensor([[0.6183, 0.6052, 0.5934, 0.6442]], dtype=torch.float64), tensor([0], dtype=torch.int8)), (tensor([[0.4142, 0.3472, 0.8108, 0.6922]], dtype=torch.float64), tensor([1], dtype=torch.int8)), (tensor([[0.4208, 0.5652, 0.6550, 0.8323]], dtype=torch.float64), tensor([2], dtype=torch.int8)), (tensor([[0.3096, 0.6196, 0.6013, 0.7608],\n",
            "        [0.6936, 0.5456, 0.6051, 0.9089]], dtype=torch.float64), tensor([0, 0], dtype=torch.int8)), (tensor([[0.5633, 0.4369, 0.6822, 0.7757]], dtype=torch.float64), tensor([0], dtype=torch.int8)), (tensor([[0.3375, 0.6775, 0.3300, 0.5217],\n",
            "        [0.3613, 0.3758, 0.4000, 0.4250],\n",
            "        [0.5981, 0.3083, 0.6863, 0.5033]], dtype=torch.float64), tensor([2, 0, 1], dtype=torch.int8)), (tensor([[0.6573, 0.5429, 0.5729, 0.8612],\n",
            "        [0.3057, 0.4867, 0.4594, 0.7301],\n",
            "        [0.1443, 0.4485, 0.2885, 0.6225]], dtype=torch.float64), tensor([0, 0, 0], dtype=torch.int8)), (tensor([[0.4851, 0.4940, 0.8513, 0.8996]], dtype=torch.float64), tensor([2], dtype=torch.int8)), (tensor([[0.3776, 0.5814, 0.5056, 0.8371],\n",
            "        [0.5712, 0.3900, 0.5952, 0.5000],\n",
            "        [0.5456, 0.4986, 0.5856, 0.7400],\n",
            "        [0.5784, 0.4371, 0.5488, 0.5829]], dtype=torch.float64), tensor([1, 1, 1, 1], dtype=torch.int8)), (tensor([[0.3297, 0.6511, 0.3500, 0.5540],\n",
            "        [0.6336, 0.6595, 0.5828, 0.4460],\n",
            "        [0.5805, 0.4341, 0.5859, 0.3549]], dtype=torch.float64), tensor([1, 1, 1], dtype=torch.int8)), (tensor([[0.3089, 0.3191, 0.3956, 0.3787],\n",
            "        [0.7489, 0.3117, 0.3733, 0.3043],\n",
            "        [0.4578, 0.7532, 0.6933, 0.2638]], dtype=torch.float64), tensor([2, 0, 1], dtype=torch.int8)), (tensor([[0.5070, 0.5509, 0.7849, 0.7185]], dtype=torch.float64), tensor([1], dtype=torch.int8)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVJWo3xbMpSh"
      },
      "source": [
        "Теперь начинается основная часть домашнего задания: обучите модель YOLO для object detection на __обучающем__ датасете. \n",
        "\n",
        " - Создайте модель и функцию ошибки YoloV1 прочитав [оригинальную статью](https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object)\n",
        " - Напишите функцию обучения модели\n",
        " - Используйте аугментации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxfMVwzHW2MJ"
      },
      "source": [
        "## Реализуйте Модель - 2 балла\n",
        "\n",
        "Копировать точное количество слоев и параметры сверток необязательно. Главное - чтобы модель работала по принципу, описанному в статье и делала предсказание в представленном формате.\n",
        "\n",
        "\n",
        "В качестве подсказки напомним, что выходом модели __для каждого обьекта__ должен быть тензор размера\n",
        "__S * S * (B * 5 + С)__, где все параметры имеют такое же значение, как и в статье: \n",
        "\n",
        "- S - количество ячеек на которое разбивается изображение по вертикали/горизонтали\n",
        "- В - количество предсказываемых прямоугольников в каждой ячейке\n",
        "- 5 - количество параметров для определения каждого прямоугольника (x_center, y_center, width, height, confidence)\n",
        "- С - количество классов (apple, banana, orange)\n",
        "\n",
        "Таким образом, мы для каждого окна размера __S x S__ предсказываем __В__ коробо и один класс"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = [ #[channels, Maxpool, kernel, padding, stride]\n",
        "    [64,True,7,3,2],\n",
        "    [192,True,3,1,1],\n",
        "    [128,False,1,0,1],\n",
        "    [256,False,3,1,1],\n",
        "    [256,False,1,0,1],\n",
        "    [512,True,3,1,1],\n",
        "    (4,\n",
        "    [[256,False,1,0,1],\n",
        "    [512,False,3,1,1]]),\n",
        "    [512,False,1,0,1],\n",
        "    [1024,True,3,1,1],\n",
        "    (2,\n",
        "    [[512,False,1,0,1],\n",
        "    [1024,False,3,1,1]]),\n",
        "    [1024,False,3,1,1],\n",
        "    [1024,False,3,1,2],\n",
        "    (2,[[1024,False,3,1,1]])\n",
        "]"
      ],
      "metadata": {
        "id": "1t_hdIBffcDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PJwrvcWW1n7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9aae0ac-e6ba-430f-9a66-045d180de171"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 637])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "class CNNBlock(nn.Module):  # можно поменять на Lightning\n",
        "    def __init__(self, in_channels, out_channels, is_max_pool:bool=False, **kwargs):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)  # в статье еще не знали про батчнорм, но мы то из будущего ...\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "        self.is_maxpool = is_max_pool  # не после каждой свертки нужно делать maxpool\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "        if self.is_maxpool:\n",
        "            x = self.maxpool(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class YOLO(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=3,in_channels:int = 3, architecture:list=params,**kwargs):\n",
        "        \"\"\"\n",
        "        :param: S * S - количество ячеек на которые разбивается изображение\n",
        "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
        "        :param: C - количество классов\n",
        "        \"\"\"\n",
        "        \n",
        "        super(YOLO, self).__init__()\n",
        "\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.in_channels=in_channels\n",
        "        self.arch = architecture\n",
        "        self.net = self._create_conv_layers()\n",
        "        self.fc = self._create_fcs(**kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.net:\n",
        "          x= block(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "    def _create_conv_layers(self):\n",
        "      in_channels = self.in_channels\n",
        "      conv = nn.ModuleList()\n",
        "      for block in self.arch:\n",
        "        if isinstance(block, list):\n",
        "          conv.append(CNNBlock(in_channels,*block[0:2],kernel_size=block[2],padding=block[3],stride=block[4]))\n",
        "          in_channels = block[0]\n",
        "        elif isinstance(block, tuple):\n",
        "          for _ in range(block[0]):\n",
        "            for small_block in block[1]:\n",
        "              conv.append(CNNBlock(in_channels,*small_block[0:2],kernel_size=small_block[2],padding=small_block[3],stride=small_block[4]))\n",
        "              in_channels=small_block[0]\n",
        "        else:\n",
        "          raise TypeError(\"architecture can include only types: 'list' or 'tuple'\")\n",
        "      return conv\n",
        "\n",
        "\n",
        "    def _create_fcs(self,p = 0.1):\n",
        "      return nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * self.S * self.S, 4096),\n",
        "            nn.Dropout(p = p),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(4096, self.S * self.S * (self.C + self.B * 5)),\n",
        "        )\n",
        "\n",
        "temp_model = YOLO()\n",
        "expected_output_shape = temp_model.S * temp_model.S * (5 * temp_model.B + temp_model.C)\n",
        "\n",
        "assert temp_model(train_dataset[0][0]).reshape(-1).shape[0] == expected_output_shape\n",
        "temp_model(train_dataset[0][0]).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJIjWKbcYUYe"
      },
      "source": [
        "## Реализуйте YoloLoss - 3 балла"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(2,2)\n",
        "x[0][0],x[0][1],x[1][0],x[1][1] = 1,2,-1,-2\n",
        "print(x)\n",
        "print(x[0])\n",
        "torch.max(x,dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1Eh6dUzB-Ld",
        "outputId": "10cc595d-4ba0-48a6-eeee-29551c102bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  2.],\n",
            "        [-1., -2.]])\n",
            "tensor([1., 2.])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([ 2., -1.]),\n",
              "indices=tensor([1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([[[1,2],[3,4]],[[-1,8],[-3,-4]]])\n",
        "z = torch.tensor([[-1,8],[-3,-4]])\n",
        "yz = torch.cat([y.unsqueeze(0),z.unsqueeze(0)],dim=0)\n",
        "print(yz)\n",
        "torch.argmax(yz,dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce59aOhNCzRr",
        "outputId": "b40a79e1-e5dc-411c-c984-bf033318b77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1,  2],\n",
            "         [ 3,  4]],\n",
            "\n",
            "        [[-1,  8],\n",
            "         [-3, -4]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.FloatTensor([[[2,2],[3,4]],[[-1,8],[-3,-4]],[[-1,8],[-3,-4]]])\n",
        "I = torch.FloatTensor([[1,0],[0,1]])\n",
        "o = torch.FloatTensor([[3,2],[1,1]])\n",
        "torch.flatten(y)\n",
        "I*y\n",
        "#y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNMI5HXQoZ-Y",
        "outputId": "810462b1-6921-461b-b8ae-c80f96640773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.,  0.],\n",
              "         [ 0.,  4.]],\n",
              "\n",
              "        [[-1.,  0.],\n",
              "         [-0., -4.]],\n",
              "\n",
              "        [[-1.,  0.],\n",
              "         [-0., -4.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = 2*torch.FloatTensor([[[1,2],[3,4]],[[-1,8],[-3,-4]]])\n",
        "torch.flatten(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO6AETZppOKw",
        "outputId": "7e4f780c-9a8c-4443-c761-f8c290e611b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.,  4.,  6.,  8., -2., 16., -6., -8.])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.flatten(z)\n",
        "float(k[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY82KmuBqoDf",
        "outputId": "852ad8b8-2071-4a05-9b98-50f4917537f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse = nn.MSELoss(reduction=\"sum\")\n",
        "mse(torch.flatten(y),torch.flatten(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUKAmgKFpkln",
        "outputId": "f7896edd-055f-41d3-c247-be818958c296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(120.)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=0\n",
        "y_0 = torch.flatten(y)\n",
        "z_0 = torch.flatten(z)\n",
        "for i in range(len(y_0)):\n",
        "  loss += (float(y_0[i])-float(z_0[i]))**2\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmP60zJgqQjp",
        "outputId": "7e6d1145-2bdc-49e0-82ac-08a6ff844131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120.0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product"
      ],
      "metadata": {
        "id": "N3su4K0n9lE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target(boxes, labels,S=7, B=2, C=3):\n",
        "  label_matrix = torch.zeros((C + 5,S, S ))\n",
        "  for box,i in enumerate(boxes):\n",
        "    x,y,w,h = box.tolist()\n",
        "    class_label = labels[i]\n",
        "    class_label = int(class_label)\n",
        "\n",
        "    i, j = int(S * y), int(S * x)\n",
        "    x_cell, y_cell = S * x - j, S * y - i\n",
        "    width_cell, height_cell = (w * S, h * S)\n",
        "\n",
        "    if label_matrix[C,i, j] == 0:\n",
        "        label_matrix[C,i, j] = 1\n",
        "        box_coordinates = torch.tensor(\n",
        "            [x_cell, y_cell, width_cell, height_cell]\n",
        "        )\n",
        "\n",
        "        label_matrix[C+1:,i, j ] = box_coordinates\n",
        "\n",
        "        # Set one hot encoding for class_label\n",
        "        label_matrix[class_label,i, j ] = 1\n",
        "\n",
        "        return label_matrix"
      ],
      "metadata": {
        "id": "ggFIjDkjId98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flat"
      ],
      "metadata": {
        "id": "BuXRWWzYw7N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwJZ7o0BYTbB"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=3):\n",
        "        \"\"\"\n",
        "        :param: S * S - количество ячеек на которые разбивается изображение\n",
        "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
        "        :param: C - количество классов\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        self.pred = predictions.reshape(self.C+5*self.B,self.S,self.S)\n",
        "        loss = 0\n",
        "        iou = torch.empty((self.B,self.S,self.S))\n",
        "        box_pred = torch.empty((4,self.S,self.S))\n",
        "        ######################################\n",
        "        ###           Cord Loss            ###\n",
        "        ######################################\n",
        "        for x,y,n in product(range(self.S),range(self.S),range(self.B)):\n",
        "          iou[n,x,y] = intersection_over_union(self.pred[self.C+5*n+1:self.C+5*n+5,x,y],self.target[self.C+1:self.C+5,...])\n",
        "        best_boxes = torch.argmax(iou,dim=0)\n",
        "        I_obj = target[self.C,...]\n",
        "        for x,y in product(range(self.S),range(self.S)):\n",
        "          box_pred[:,x,y]=self.pred[self.C+5*int(best_boxes[x,y])+1:self.C+5*int(best_boxes[x,y])+5,x, y]\n",
        "        box_pred = I_obj*box_pred\n",
        "        box_targ = I_obj * target[self.C+1:self.C+5,...]\n",
        "        box_pred[2:4,...],box_targ[2:4,...] = torch.sqrt(torch.abs(box_pred[2:4,...])),torch.sqrt(torch.abs(box_targ[2:4,...])) \n",
        "        loss +=self.lambda_coord*self.mse(torch.flatten(box_pred),torch.flatten(box_targ))\n",
        "        ######################################\n",
        "        ###            Obj Loss            ###\n",
        "        ######################################\n",
        "        obj_pred = torch.empty((self.S,self.S))\n",
        "        for x,y in product(range(self.S),range(self.S)):\n",
        "          obj_pred[x,y]=self.pred[self.C+5*int(best_boxes[x,y]),x, y]\n",
        "        obj_pred = I_obj*obj_pred\n",
        "        obj_targ = I_obj* target[self.C,...]\n",
        "        loss += self.mse(torch.flatten(obj_pred),torch.flatten(obj_targ))\n",
        "        ######################################\n",
        "        ###           No Obj Loss          ###\n",
        "        ######################################\n",
        "        I_noobj = 1-I_obj\n",
        "        noobj_targ = I_noobj*target[self.C,...]\n",
        "        for i in range(self.B):\n",
        "          noobj_pred = I_noobj*self.pred[self.C+5*i,x, y]\n",
        "          loss += self.lambda_noobj*self.mse(torch.flatten(noobj_pred),torch.flatten(noobj_targ))\n",
        "        ######################################\n",
        "        ###           Class Loss           ###\n",
        "        ######################################\n",
        "        loss += self.mse(\n",
        "            torch.flatten(I_obj * self.pred[:self.C,...]),\n",
        "            torch.flatten(I_obj * target[:self.C,... ] ),\n",
        "        )\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c=128\n",
        "x = torch.zeros((3,c,c))\n",
        "for i in range(c):\n",
        "  x[1,c-1,i]=1\n",
        "  x[2,c-1,i]=1\n",
        "  x[0,c-1,i]=1\n",
        "  x[1,i,c-1]=1\n",
        "  x[2,i,c-1]=1\n",
        "  x[0,i,c-1]=1\n",
        "#x.size()\n",
        "weights = torch.ones((3,3,3,3))\n",
        "p = torch.nn.Conv2d(3,3,kernel_size=3,stride=2)\n",
        "with torch.no_grad():\n",
        "    p.weight = nn.Parameter(weights)\n",
        "y = p(x)\n",
        "print(x)\n",
        "print(y)\n",
        "y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmIMnECG7V7t",
        "outputId": "46c891ab-bdb9-4f32-c432-45968bffd429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
            "tensor([[[-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         ...,\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875]],\n",
            "\n",
            "        [[ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         ...,\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751]],\n",
            "\n",
            "        [[-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         ...,\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422]]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 63, 63])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m=nn.MaxPool2d(2)\n",
        "z = m(y)\n",
        "print(z)\n",
        "z.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMP374c_BFBw",
        "outputId": "adf92fc2-1804-4168-83a9-872012682c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         ...,\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875],\n",
            "         [-0.0875, -0.0875, -0.0875,  ..., -0.0875, -0.0875, -0.0875]],\n",
            "\n",
            "        [[ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         ...,\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751],\n",
            "         [ 0.0751,  0.0751,  0.0751,  ...,  0.0751,  0.0751,  0.0751]],\n",
            "\n",
            "        [[-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         ...,\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422],\n",
            "         [-0.1422, -0.1422, -0.1422,  ..., -0.1422, -0.1422, -0.1422]]],\n",
            "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 31, 31])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights1 = torch.ones((3,3,5,5))\n",
        "k = torch.nn.Conv2d(3,3,kernel_size=5,stride=2)\n",
        "with torch.no_grad():\n",
        "    k.weight = nn.Parameter(weights1)\n",
        "r = k(z)\n",
        "#print(x)\n",
        "print(r)\n",
        "r.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_CJCsw9BTYS",
        "outputId": "f5b53716-74d2-417f-f26c-af2a59c7e257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719]],\n",
            "\n",
            "        [[-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934],\n",
            "         [-3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934,\n",
            "          -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934, -3.7934]],\n",
            "\n",
            "        [[-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719],\n",
            "         [-3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719,\n",
            "          -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719, -3.7719]]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 14, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "float(x[1,0,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANDbWmRUAGbJ",
        "outputId": "dbb259e8-69d2-443a-b9bf-335e6661115d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5978139209498624.0"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ1eev1EeNk7"
      },
      "source": [
        "## Реализуйте дополнительные функции из статьи - 2 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMF8e6yXU6QV"
      },
      "outputs": [],
      "source": [
        "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
        "    ## YOUR CODE\n",
        "    pass\n",
        "\n",
        "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5):\n",
        "    ## YOUR CODE\n",
        "    pass\n",
        "\n",
        "def get_bound_boxes(loader, model, iou_threshold=.5, threshold=.4):\n",
        "    ## YOUR CODE\n",
        "    return all_pred_boxes, all_true_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z38hYLM6haDk"
      },
      "source": [
        "## Обучите модель и посчитайте метрики для задачи детекции - 2 балла \n",
        "\n",
        "Несмотря на то, что в этом блоке ничего сильно нового для вас не ожидается и за него формально дается лишь два балла - провести обучение очень важно для понимания того, насколько правильно реализована ваша модель и лосс.\n",
        "\n",
        "В процессе обучения будет видно все ли размерности совпадают, падает ли лосс и растут ли метрики целевой задачи, поэтому на практике этот пункт гораздо оказывается гораздо важнее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BTNHNqtMpSi"
      },
      "outputs": [],
      "source": [
        "class YOLOLearner(pl.LightningModule):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = YOLO()\n",
        "        self.loss = YoloLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx) -> torch.Tensor:\n",
        "        ## YOUR CODE\n",
        "        pass\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx) -> None:\n",
        "        ## YOUR CODE\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRl42I2xMpSi"
      },
      "outputs": [],
      "source": [
        "model = # YOUR CODE\n",
        "n_epochs = # YOUR CODE\n",
        "\n",
        "yolo_learner = YOLOLearner(...)  ## YOUR CODE\n",
        "\n",
        "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "trainer = pl.Trainer(accelerator=device, max_epochs=n_epochs)\n",
        "\n",
        "trainer.fit(yolo_learner, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb7ioohR96vu"
      },
      "source": [
        "## Посчитайте метрики задачи детекции на валидационной выборке\n",
        "\n",
        "Попробуйте понять насколько хороши ваши показатели. Если числа кажутся подозрительно низкими - возможно вам стоит перепроверить ваше решение. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUnlNeot98un"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_YG71pYMpSi"
      },
      "source": [
        "## Визуализируйте предсказанные bounding box'ы для любых пяти картинок из __валидационного__ датасета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgVdVzvMMpSi"
      },
      "outputs": [],
      "source": [
        "image, targets = next(iter(val_dataset))\n",
        "preds = ## YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpp4jHs0MpSi"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageDraw\n",
        "\n",
        "image = torchvision.transform.ToPILImage()(image)\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "for box in targets[0]:\n",
        "    ## YOUR CODE\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
        "\n",
        "for box in preds[0]:\n",
        "    ## YOUR CODE\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "z38hYLM6haDk",
        "eb7ioohR96vu",
        "o_YG71pYMpSi"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}