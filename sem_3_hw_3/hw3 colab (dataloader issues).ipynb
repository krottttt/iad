{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKdcvMtSMpSg"
      },
      "source": [
        "#Майнор ИАД. Домашнее задание 3. YOLO.\n",
        "\n",
        "В этом задании вы напишете и обучите свой собственный YOLO детектор. Нужно будет разобраться со статьей: понять какого формата должна быть обучающая пара (x, y), как перевести лосс из математической формулы в питоновский код - ну и конечно понять и реализовать саму архитектуру модели.\n",
        "\n",
        "Выборка на котрой мы будем обучать модель состоит из разнообразных фотографий яблок, бананов и апельсинов. Данные скачиваем [отсюда](https://drive.google.com/file/d/1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3/view?usp=sharing).\n",
        "\n",
        "Баллы за ДЗ распределены следующим образом: \n",
        "- Выборка для YoloV1 - 2 балла\n",
        "- YOLO модель - 2 балла\n",
        "- YOLO Loss - 3 балла\n",
        "- Вспомогательные функции - 2 балла\n",
        "- Обучение и расчет метрик - 2 балла\n",
        "\n",
        "Для построения и обучения можно использовать как pytorch, так и pytorch-lightning.\n",
        "\n",
        "Да-да, баллов в сумме получается 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY4niK1xMpSg",
        "outputId": "09322d23-4322-4db5-e832-8c15a0ca4e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.8.3.post1-py3-none-any.whl (798 kB)\n",
            "\u001b[K     |████████████████████████████████| 798 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 47.5 MB/s \n",
            "\u001b[?25hCollecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.12.1+cu113)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.1.1)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.9.24)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=f9b3f275bc8f5a8b8f6ebaa72143e128bf50054d7ec368acfc0e4ffda5685297\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/10/06/2a990ee4d73a8479fe2922445e8a876d38cfbfed052284c6a1\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, torchmetrics, tensorboardX, lightning-utilities, xmltodict, pytorch-lightning\n",
            "Successfully installed fire-0.4.0 lightning-utilities-0.3.0 pytorch-lightning-1.8.3.post1 tensorboardX-2.5.1 torchmetrics-0.11.0 xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "# Данная библиотека понадобится нам, чтобы обработать разметку\n",
        "! pip install xmltodict pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNSQ7FNss30F"
      },
      "source": [
        "Скачаем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN1dE6eY7PjV",
        "outputId": "7c065adc-df0c-4649-cf6f-9f876f37e68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "drwx------ 4 root root 4096 Nov 26 08:32 data\n",
            "drwxr-xr-x 3 root root 4096 Dec  3 18:27 __MACOSX\n",
            "drwxr-xr-x 1 root root 4096 Dec  1 20:08 sample_data\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3\" -O data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -q data.zip\n",
        "!rm data.zip\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep38vdW_s-Rz"
      },
      "source": [
        "Посмотрим как выглядит один из файлов разметки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqwaHl3ntBaN",
        "outputId": "47967c86-fd6c-4802-aedf-1015cdd3a2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<annotation>\n",
            "\t<folder>train</folder>\n",
            "\t<filename>apple_3.jpg</filename>\n",
            "\t<path>C:\\tensorflow1\\models\\research\\object_detection\\images\\train\\apple_3.jpg</path>\n",
            "\t<source>\n",
            "\t\t<database>Unknown</database>\n",
            "\t</source>\n",
            "\t<size>\n",
            "\t\t<width>1000</width>\n",
            "\t\t<height>708</height>\n",
            "\t\t<depth>3</depth>\n",
            "\t</size>\n",
            "\t<segmented>0</segmented>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>1</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>584</xmin>\n",
            "\t\t\t<ymin>438</ymin>\n",
            "\t\t\t<xmax>867</xmax>\n",
            "\t\t\t<ymax>708</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>492</xmin>\n",
            "\t\t\t<ymin>141</ymin>\n",
            "\t\t\t<xmax>740</xmax>\n",
            "\t\t\t<ymax>394</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>176</xmin>\n",
            "\t\t\t<ymin>199</ymin>\n",
            "\t\t\t<xmax>490</xmax>\n",
            "\t\t\t<ymax>466</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>367</xmin>\n",
            "\t\t\t<ymin>17</ymin>\n",
            "\t\t\t<xmax>619</xmax>\n",
            "\t\t\t<ymax>240</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>642</xmin>\n",
            "\t\t\t<ymin>35</ymin>\n",
            "\t\t\t<xmax>907</xmax>\n",
            "\t\t\t<ymax>269</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "</annotation>\n"
          ]
        }
      ],
      "source": [
        "!cat data/train/apple_3.xml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdQxrA5_MpSg"
      },
      "source": [
        "## Релизуйте выборку для YoloV1 - 2 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QXG9reop-BkS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "import tqdm\n",
        "import xmltodict\n",
        "\n",
        "from IPython.core.display import struct\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import albumentations as A\n",
        "import albumentations.pytorch\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import auc\n",
        "# Добавьте необходимые вам библиотеки, если их не окажется в списке выше"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = glob.glob(os.path.join('data/train/','*.jpg'))"
      ],
      "metadata": {
        "id": "iDnmmrDnII6_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fileptr = open(\"/content/data/train/orange_76.xml\",\"r\")\n",
        "\n",
        "#read xml content from the file\n",
        "xml_content= fileptr.read()\n",
        "\n",
        "#change xml format to ordered dict\n",
        "my_ordered_dict=xmltodict.parse(xml_content)\n",
        "my_ordered_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6KKk6FORRzw",
        "outputId": "cbdd61b5-ed44-46dd-94c4-3f7f974c1e73"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'annotation': {'folder': 'train',\n",
              "  'filename': 'orange_76.jpg',\n",
              "  'path': 'C:\\\\tensorflow1\\\\models\\\\research\\\\object_detection\\\\images\\\\train\\\\orange_76.jpg',\n",
              "  'source': {'database': 'Unknown'},\n",
              "  'size': {'width': '1620', 'height': '1080', 'depth': '3'},\n",
              "  'segmented': '0',\n",
              "  'object': [{'name': 'orange',\n",
              "    'pose': 'Unspecified',\n",
              "    'truncated': '1',\n",
              "    'difficult': '0',\n",
              "    'bndbox': {'xmin': '494', 'ymin': '1', 'xmax': '1452', 'ymax': '741'}},\n",
              "   {'name': 'orange',\n",
              "    'pose': 'Unspecified',\n",
              "    'truncated': '1',\n",
              "    'difficult': '0',\n",
              "    'bndbox': {'xmin': '822',\n",
              "     'ymin': '643',\n",
              "     'xmax': '1620',\n",
              "     'ymax': '1066'}}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(my_ordered_dict['annotation']['object'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42HlGET9SDlI",
        "outputId": "0de9e021-7cf4-463e-c83f-4d312998c5e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_t = xmltodict.parse(open(\"/content/data/train/orange_76.xml\",\"r\").read())['annotation']['object']\n",
        "dic_t[0]['bndbox']['xmax']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-KxcEtLuUDdp",
        "outputId": "eecdaa4d-4cf2-41e8-9db1-e23b61402da9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1452'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im = np.array(Image.open('/content/data/train/mixed_8.jpg').convert(\"RGB\"))\n",
        "im.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm1jlm4nVsrQ",
        "outputId": "b9cfac6b-e971-4132-b3c7-a4370425eafe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "757"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "metadata": {
        "id": "_XL3M6chIOee"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([A.ToFloat (max_value=None, always_apply=False, p=1.0),\n",
        "    A.pytorch.transforms.ToTensorV2()\n",
        "                       ], \n",
        "                             bbox_params=A.BboxParams(format='yolo',\n",
        "                                                      label_fields=['class_labels']))\n",
        "train_dataset = FruitDataset(transforms=transform,\n",
        "    data_dir=\"./data/train\")\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=1)\n",
        "test_dataset = FruitDataset(transforms=transform,\n",
        "    data_dir=\"./data/test\")\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "PjfM6pjpH9qm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_and_std(dataloader):\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "    for data, _ in dataloader:\n",
        "        # Mean over batch, height and width, but not over the channels\n",
        "        channels_sum += torch.mean(data, dim=[0,2,3])\n",
        "        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n",
        "        num_batches += 1\n",
        "    \n",
        "    mean = channels_sum / num_batches\n",
        "\n",
        "    # std = sqrt(E[X^2] - (E[X])^2)\n",
        "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
        "\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "NCm-QYa_IAkY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = get_mean_and_std(train_dataloader)\n",
        "mean_tr,std_tr = mean.tolist(),std.tolist()\n",
        "mean, std = get_mean_and_std(test_dataloader)\n",
        "mean_test,std_test = mean.tolist(),std.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4w0K7-DIBwW",
        "outputId": "01db8e59-138a-4c25-994c-3782317514a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmvn8pI5XPYh",
        "outputId": "c40fc41e-70ca-4b4c-c142-a8dec4341d5c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8134564757347107, 0.6914445757865906, 0.5386325120925903]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "std_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET1UEyc4XRyn",
        "outputId": "09b5ad16-03d3-46d3-b87c-9bcb00e9c4fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2758112847805023, 0.3231792151927948, 0.40659797191619873]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGM42OVNYtmy",
        "outputId": "f2ee3bd7-540a-43a3-c147-658b1093967f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8366213440895081, 0.7004528641700745, 0.5619865655899048]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "std_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCtTASZxYu6t",
        "outputId": "099a9e43-32f6-472b-8eef-f3d73898c008"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.25586485862731934, 0.33195656538009644, 0.40674328804016113]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(Image.open(\"/content/data/train/apple_24.jpg\").convert(\"RGB\")).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkcRta0XRuL5",
        "outputId": "02303279-a2a5-486f-a1cd-01cdf9a6fcfc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1696, 1132, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.splitext(\"/content/data/train/apple_24.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc_VTIuTTb5M",
        "outputId": "815701b4-aa50-4d39-f4a2-5c0ce62b4c9e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/data/train/apple_24', '.jpg')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gL8_CyyTYJ-"
      },
      "source": [
        "Так как в этом домашнем задании использовать аугментации для обучения __обязательно__ - советуем воспользоваться библиотекой albumentations.\n",
        "\n",
        "Она  особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Для знакомства с этим механизмом советуем следующий [гайд](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). \n",
        "\n",
        "Вы все еще можете избрать путь torchvision.transforms, вам потребуется знакомый нам метод `__getitem__`, однако вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную\n",
        "\n",
        "__Обратите внимание__ на то, что в статье коробки предсказаний параметризуются через: _(x_center, y_center, width, height)_ (причем эти значения _относительные_), а в наших файлах - это _(x_min, y_min, x_max, y_max)_\n",
        "\n",
        "Также, помните что модель должна предсказывать как прямоугольник с обьектом, так и вероятности каждого класса!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "tjZkU0vzMpSh"
      },
      "outputs": [],
      "source": [
        "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
        "\n",
        "class FruitDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.image_paths = glob.glob(\n",
        "            os.path.join(data_dir,'*.jpg')\n",
        "        )\n",
        "        self.box_paths = [x+\".xml\" for x in [os.path.splitext(x)[0] for x in self.image_paths]]\n",
        "        assert len(self.image_paths) == len(self.box_paths)\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # Координаты прямоугольников советуем вернуть именно в формате (x_center, y_center, width, height)\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
        "        boxes, class_labels = self.__get_boxes_from_xml(self.box_paths[idx],image)\n",
        "        print(boxes,type(boxes))\n",
        "        print(class_labels,type(class_labels))\n",
        "        if self.transforms is not None:\n",
        "          augs = self.transforms(image=image, bboxes = boxes, class_labels=class_labels)\n",
        "          image = augs['image']\n",
        "          boxes = augs['bboxes']\n",
        "          class_labels = augs['class_labels']\n",
        "        return image, (boxes, class_labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __get_boxes_from_xml(self, xml_filename: str,image):\n",
        "      \"\"\"\n",
        "      Метод, который считает и распарсит (с помощью xmltodict) переданный xml\n",
        "      файл и вернет координаты прямоугольников обьектов на соответсвующей фотографии\n",
        "      и название класса обьекта в каждом прямоугольнике\n",
        "\n",
        "      Обратите внимание, что обьектов может быть как несколько, так и один единственный\n",
        "      \"\"\"\n",
        "      boxes = []\n",
        "      class_labels = []\n",
        "      dic = xmltodict.parse(open(xml_filename,\"r\").read())['annotation']\n",
        "      im_w = int(dic['size']['width'])\n",
        "      if im_w == 0:\n",
        "        im_w = image.shape[1]\n",
        "      im_h = int(dic['size']['height'])\n",
        "      if im_h == 0:\n",
        "        im_h = image.shape[0]\n",
        "      obj = dic['object']\n",
        "      if isinstance(obj, dict):\n",
        "        obj = [obj]\n",
        "      for item in obj:\n",
        "        boxes.append(self.__convert_to_yolo_box_params(\n",
        "            [int(x) for x in [item['bndbox']['xmin'], item['bndbox']['ymin'], item['bndbox']['xmax'], item['bndbox']['ymax']]],\n",
        "            im_w,im_h)\n",
        "        ) \n",
        "        class_labels.append(item['name'])      \n",
        "      return boxes,class_labels\n",
        "\n",
        "    def __convert_to_yolo_box_params(self, box_coordinates: List[int], im_w, im_h):\n",
        "      \"\"\"\n",
        "      Перейти от [xmin, ymin, xmax, ymax] к [x_center, y_center, width, height].\n",
        "      \n",
        "      Обратите внимание, что параметры [x_center, y_center, width, height] - это\n",
        "      относительные значение в отрезке [0, 1]\n",
        "\n",
        "      :param: box_coordinates - координаты коробки в формате [xmin, ymin, xmax, ymax]\n",
        "      :param: im_w - ширина исходного изображения\n",
        "      :param: im_h - высота исходного изображения\n",
        "\n",
        "      :return: координаты коробки в формате [x_center, y_center, width, height]\n",
        "      \"\"\"\n",
        "      ans = [] \n",
        "      ans.append((box_coordinates[0] + box_coordinates[2]) / 2 / im_w)# x_center  \n",
        "      ans.append((box_coordinates[1] + box_coordinates[3]) / 2 / im_h)  # y_center\n",
        "      ans.append((box_coordinates[2] - box_coordinates[0]) / im_w)# width  \n",
        "      ans.append((box_coordinates[3] - box_coordinates[1]) / im_h)  # height \n",
        "      return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "OwXeSiAjdGeq"
      },
      "outputs": [],
      "source": [
        "WIDTH, HEIGHT = 512,512\n",
        "\n",
        "train_transform = A.Compose([A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
        "    A.GaussNoise (var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n",
        "    A.Normalize (mean=mean_tr, std=std_tr, max_pixel_value=255.0, always_apply=False, p=1.0),A.Resize(WIDTH, HEIGHT),\n",
        "    A.pytorch.transforms.ToTensorV2(), ], \n",
        "                             bbox_params=A.BboxParams(format='yolo',\n",
        "                                                      label_fields=['class_labels']))\n",
        "test_transform = A.Compose([A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
        "    A.GaussNoise (var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n",
        "    A.Normalize (mean=mean_test, std=std_test, max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "    A.pytorch.transforms.ToTensorV2()],\n",
        "                            bbox_params=A.BboxParams(format='yolo',\n",
        "                                                     label_fields=['class_labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayPwbRKocdCE",
        "outputId": "4e8b7f88-66c2-4991-d34d-237b0ba3acbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5, 0.589041095890411, 0.9223744292237442, 0.6575342465753424]] <class 'list'>\n",
            "['banana'] <class 'list'>\n",
            "[[0.5, 0.589041095890411, 0.9223744292237442, 0.6575342465753424]] <class 'list'>\n",
            "['banana'] <class 'list'>\n",
            "[[0.5, 0.589041095890411, 0.9223744292237442, 0.6575342465753424]] <class 'list'>\n",
            "['banana'] <class 'list'>\n",
            "[[0.5, 0.589041095890411, 0.9223744292237442, 0.6575342465753424]] <class 'list'>\n",
            "['banana'] <class 'list'>\n",
            "[[0.5, 0.589041095890411, 0.9223744292237442, 0.6575342465753424]] <class 'list'>\n",
            "['banana'] <class 'list'>\n",
            "Тесты успешно пройдены\n"
          ]
        }
      ],
      "source": [
        "train_dataset = FruitDataset(\n",
        "    transforms=train_transform,\n",
        "    data_dir=\"./data/train\"\n",
        "    )\n",
        "\n",
        "val_dataset = FruitDataset(\n",
        "    transforms=test_transform, \n",
        "    data_dir=\"./data/test\"\n",
        "    )\n",
        "\n",
        "# Немного проверок, чтобы убедиться в правильности направления решения\n",
        "assert isinstance(train_dataset[2], tuple)\n",
        "assert len(train_dataset[2]) == 2\n",
        "assert isinstance(train_dataset[2][0], torch.Tensor)\n",
        "assert isinstance(train_dataset[2][1], tuple)\n",
        "assert len(train_dataset[2][1]) == 2\n",
        "print(\"Тесты успешно пройдены\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,data in enumerate(train_dataset):\n",
        "  print(i)\n",
        "  #print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "HyvlAAQQSBw3",
        "outputId": "f43d47af-7f72-4b9f-b169-300e5efe9846"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.40515222 0.53703704 0.53395785 0.66381766]]] <class 'numpy.ndarray'>\n",
            "[['orange']] <class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-7a837df0e7a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-4a73b12f1058>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           \u001b[0maugs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m           \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bboxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mcheck_and_convert\u001b[0;34m(self, data, rows, cols, direction)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"from\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_from_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/albumentations/core/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_to_albumentations\u001b[0;34m(self, data, rows, cols)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_bboxes_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/albumentations/core/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_bboxes_to_albumentations\u001b[0;34m(bboxes, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    392\u001b[0m ) -> List[Tuple]:\n\u001b[1;32m    393\u001b[0m     \u001b[0;34m\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_bbox_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/albumentations/core/bbox_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    392\u001b[0m ) -> List[Tuple]:\n\u001b[1;32m    393\u001b[0m     \u001b[0;34m\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_bbox_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/albumentations/core/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_bbox_to_albumentations\u001b[0;34m(bbox, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_validity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_bbox\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_bbox\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In YOLO format all coordinates must be float and in range (0, 1]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "9V1Tl_GAdeIv"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size= 4,\n",
        "    shuffle=True)\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=4, \n",
        "    shuffle=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fRR9ns6MpSh"
      },
      "source": [
        "Теперь определим функцию для рассчета Intersection Over Union по 4 углам двух прямоугольников"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Rd88hnZiMpSh"
      },
      "outputs": [],
      "source": [
        "def intersection_over_union(predicted_bbox, gt_bbox) -> float:\n",
        "    \"\"\"\n",
        "    Intersection Over Union для двух прямоугольников\n",
        "\n",
        "    :param: predicted_bbox - [x_min, y_min, x_max, y_max]\n",
        "    :param: gt_bbox - [x_min, y_min, x_max, y_max]\n",
        "    \n",
        "    :return: Intersection Over Union\n",
        "    \"\"\"\n",
        "\n",
        "    intersection_bbox = np.array(\n",
        "        [\n",
        "            max(predicted_bbox[0], gt_bbox[0]),\n",
        "            max(predicted_bbox[1], gt_bbox[1]),\n",
        "            min(predicted_bbox[2], gt_bbox[2]),\n",
        "            min(predicted_bbox[3], gt_bbox[3]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
        "        intersection_bbox[3] - intersection_bbox[1], 0\n",
        "    )\n",
        "    area_dt = (predicted_bbox[2] - predicted_bbox[0]) * (predicted_bbox[3] - predicted_bbox[1])\n",
        "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
        "\n",
        "    union_area = area_dt + area_gt - intersection_area\n",
        "\n",
        "    iou = intersection_area / union_area\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in train_dataloader:\n",
        "  print(len(x[1]))"
      ],
      "metadata": {
        "id": "-6nho35Q_NLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = next(iter(train_dataloader))\n",
        "print(x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "x4AzwdwRCtb6",
        "outputId": "a072a3aa-65a5-445c-9baf-39104c79f299"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6495726495726496, 0.5104166666666666, 0.49572649572649574, 0.7291666666666666], [0.26495726495726496, 0.44166666666666665, 0.42735042735042733, 0.7]] <class 'list'>\n",
            "['apple', 'apple'] <class 'list'>\n",
            "[[0.47244897959183674, 0.5705882352941176, 0.5489795918367347, 0.7764705882352941]] <class 'list'>\n",
            "['apple'] <class 'list'>\n",
            "[[0.7542307692307693, 0.2884451996601529, 0.41923076923076924, 0.4392523364485981]] <class 'list'>\n",
            "['orange'] <class 'list'>\n",
            "[[0.54453125, 0.5864197530864198, 0.8015625, 0.7901234567901234]] <class 'list'>\n",
            "['orange'] <class 'list'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-e5262d59e406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0melem_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# It may be accessed twice, so we use a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YYyFU_POExP",
        "outputId": "a1260427-0c79-4fb2-b525-a4e98081cc30"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([(0.594847775175644,\n",
              "   0.5370370370370371,\n",
              "   0.5339578454332554,\n",
              "   0.6638176638176638)],\n",
              " ['orange'])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVJWo3xbMpSh"
      },
      "source": [
        "Теперь начинается основная часть домашнего задания: обучите модель YOLO для object detection на __обучающем__ датасете. \n",
        "\n",
        " - Создайте модель и функцию ошибки YoloV1 прочитав [оригинальную статью](https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object)\n",
        " - Напишите функцию обучения модели\n",
        " - Используйте аугментации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxfMVwzHW2MJ"
      },
      "source": [
        "## Реализуйте Модель - 2 балла\n",
        "\n",
        "Копировать точное количество слоев и параметры сверток необязательно. Главное - чтобы модель работала по принципу, описанному в статье и делала предсказание в представленном формате.\n",
        "\n",
        "\n",
        "В качестве подсказки напомним, что выходом модели __для каждого обьекта__ должен быть тензор размера\n",
        "__S * S * (B * 5 + С)__, где все параметры имеют такое же значение, как и в статье: \n",
        "\n",
        "- S - количество ячеек на которое разбивается изображение по вертикали/горизонтали\n",
        "- В - количество предсказываемых прямоугольников в каждой ячейке\n",
        "- 5 - количество параметров для определения каждого прямоугольника (x_center, y_center, width, height, confidence)\n",
        "- С - количество классов (apple, banana, orange)\n",
        "\n",
        "Таким образом, мы для каждого окна размера __S x S__ предсказываем __В__ коробо и один класс"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PJwrvcWW1n7"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(nn.Module):  # можно поменять на Lightning\n",
        "    def __init__(self, in_channels, out_channels, is_max_pool:bool=False, **kwargs):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)  # в статье еще не знали про батчнорм, но мы то из будущего ...\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "        self.is_maxpool = is_max_pool  # не после каждой свертки нужно делать maxpool\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "        if self.is_maxpool:\n",
        "            x = self.maxpool(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class YOLO(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=3):\n",
        "        \"\"\"\n",
        "        :param: S * S - количество ячеек на которые разбивается изображение\n",
        "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
        "        :param: C - количество классов\n",
        "        \"\"\"\n",
        "        \n",
        "        super(YOLO, self).__init__()\n",
        "\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        ## YOUR CODE\n",
        "        \n",
        "        ## YOUR CODE\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        ## YOUR CODE\n",
        "        \n",
        "        pass\n",
        "\n",
        "\n",
        "temp_model = YOLO()\n",
        "expected_output_shape = temp_model.S * temp_model.S * (5 * temp_model.B + temp_model.C)\n",
        "\n",
        "assert temp_model(train_dataset[0][0]).reshape(-1).shape[0] == expected_output_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJIjWKbcYUYe"
      },
      "source": [
        "## Реализуйте YoloLoss - 3 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwJZ7o0BYTbB"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=3):\n",
        "        \"\"\"\n",
        "        :param: S * S - количество ячеек на которые разбивается изображение\n",
        "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
        "        :param: C - количество классов\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        ## YOUR CODE\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ1eev1EeNk7"
      },
      "source": [
        "## Реализуйте дополнительные функции из статьи - 2 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMF8e6yXU6QV"
      },
      "outputs": [],
      "source": [
        "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
        "    ## YOUR CODE\n",
        "    pass\n",
        "\n",
        "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5):\n",
        "    ## YOUR CODE\n",
        "    pass\n",
        "\n",
        "def get_bound_boxes(loader, model, iou_threshold=.5, threshold=.4):\n",
        "    ## YOUR CODE\n",
        "    return all_pred_boxes, all_true_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z38hYLM6haDk"
      },
      "source": [
        "## Обучите модель и посчитайте метрики для задачи детекции - 2 балла \n",
        "\n",
        "Несмотря на то, что в этом блоке ничего сильно нового для вас не ожидается и за него формально дается лишь два балла - провести обучение очень важно для понимания того, насколько правильно реализована ваша модель и лосс.\n",
        "\n",
        "В процессе обучения будет видно все ли размерности совпадают, падает ли лосс и растут ли метрики целевой задачи, поэтому на практике этот пункт гораздо оказывается гораздо важнее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BTNHNqtMpSi"
      },
      "outputs": [],
      "source": [
        "class YOLOLearner(pl.LightningModule):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = YOLO()\n",
        "        self.loss = YoloLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx) -> torch.Tensor:\n",
        "        ## YOUR CODE\n",
        "        pass\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx) -> None:\n",
        "        ## YOUR CODE\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRl42I2xMpSi"
      },
      "outputs": [],
      "source": [
        "model = # YOUR CODE\n",
        "n_epochs = # YOUR CODE\n",
        "\n",
        "yolo_learner = YOLOLearner(...)  ## YOUR CODE\n",
        "\n",
        "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "trainer = pl.Trainer(accelerator=device, max_epochs=n_epochs)\n",
        "\n",
        "trainer.fit(yolo_learner, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb7ioohR96vu"
      },
      "source": [
        "## Посчитайте метрики задачи детекции на валидационной выборке\n",
        "\n",
        "Попробуйте понять насколько хороши ваши показатели. Если числа кажутся подозрительно низкими - возможно вам стоит перепроверить ваше решение. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUnlNeot98un"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_YG71pYMpSi"
      },
      "source": [
        "## Визуализируйте предсказанные bounding box'ы для любых пяти картинок из __валидационного__ датасета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgVdVzvMMpSi"
      },
      "outputs": [],
      "source": [
        "image, targets = next(iter(val_dataset))\n",
        "preds = ## YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpp4jHs0MpSi"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageDraw\n",
        "\n",
        "image = torchvision.transform.ToPILImage()(image)\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "for box in targets[0]:\n",
        "    ## YOUR CODE\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
        "\n",
        "for box in preds[0]:\n",
        "    ## YOUR CODE\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}